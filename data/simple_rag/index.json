{
  "12": {
    "file_name": "12.pdf",
    "file_path": "/Users/patrickcheung/Downloads/12.pdf",
    "chunks": [
      "Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n12 Model Alignment, Prompting,\nand In-Context Learning\n“Hal,” said Bowman, now speaking with an icy calm. “I am not incapaci-\ntated. Unlessyouobeymyinstructions,Ishallbeforcedtodisconnectyou.”\nArthurC.Clarke\nInthischapterweshowhowtogetLLMstodotasksforussimplybytalkingto\nthem.",
      "whowtogetLLMstodotasksforussimplybytalkingto\nthem. TogetanLLMtotranslateasentence, outlineatalk, ordraftaworkemail,\nwe’llsimplydescribewhatwewantinnaturallanguage. Wecalltheseinstructions\nprompts wegivetolanguagemodelsprompts.\nPromptingreliesoncontextualgeneration. Giventhepromptascontext,thelan-\nguagemodelgeneratesthenexttokenbasedonitstokenprobability,conditionedon\ntheprompt:P(w\ni\n|w<i ).Apromptcanbeaquestion(like“Whatisatransformernet-\nwork?",
      "ptcanbeaquestion(like“Whatisatransformernet-\nwork?”), possibly in a structured format (like “Q: What is a transformer network?\nA:”), or can be an instruction (like “Translate the following sentence into Hindi:\ndemonstrations ‘Chopthegarlicfinely’”). Apromptcanalsocontaindemonstrations,examplesto\nhelp make the instructions clearer, (like “Give the sentiment of the following sen-\ntence.ExampleInput:“IreallylovedTaishanCuisine.”Output:positive”.",
      "put:“IreallylovedTaishanCuisine.”Output:positive”.)Aswe’ll\nsee,promptingcanbeappliedtoinherentlygenerativetasks(likesummarizationand\ntranslation)aswellastoonesmorenaturallythoughtofasclassificationtasks.\nPrompts get language models to generate text, but they also can be viewed as\na learning signal, because these demonstrations can help language models learn\nto perform novel tasks. For this reason we also refer to prompting as in-context-",
      "reason we also refer to prompting as in-context-\nin-context- learning—learningthatimprovesmodelperformanceorreducessomelossbutdoes\nlearning\nnotinvolvegradient-basedupdatestothemodel’sunderlyingparameters.\nButLLMsaswe’vedescribedthemsofarturnouttobebadatfollowinginstruc-\ntions. Pretrainingisn’tsufficienttomakethemhelpful. We’llintroduceinstruction\ninstruction tuning, a technique that helps LLMs learn to correctly respond to instructions by\ntuning",
      "rn to correctly respond to instructions by\ntuning\nfinetuningthemonacorpusofinstructionswiththeircorrespondingresponse.\nA second failure of LLMs is that they can be harmful: their pretraining isn’t\nsufficienttomakethemsafe. ReaderswhoknowArthurC.Clarke’s2001: ASpace\nOdysseyortheStanleyKubrickfilmknowthatthequoteabovecomesinthecontext\nthattheartificialintelligenceHalbecomesparanoidandtriestokillthecrewofthe\nspaceship.",
      "comesparanoidandtriestokillthecrewofthe\nspaceship. UnlikeHal, languagemodelsdon’thaveintentionalityormentalhealth\nissueslikeparanoidthinking,buttheydohavethecapacityforharm.Pretrainedlan-\nguagemodelscansaythingsthataredangerousorfalse(likegivingunsafemedical\nadvice)andtheycanverballyattackusersorsaytoxicorhatefulthings.\nDealingwithsafetycanbedonepartlybyaddingsafetytrainingintoinstruction\ntuning. Butanimportantaspectofsafetytrainingisasecondtechnique,preference",
      "pectofsafetytrainingisasecondtechnique,preference\npreference alignment(oftenimplemented,aswe’llsee,withtheRLHForDPOalgorithms)in\nalignment\nwhichaseparatemodelistrainedtodecidehowmuchacandidateresponsealigns\nwith human preferences. Together we refer to instruction tuning and preference\nmodel alignmentasmodelalignment.Theintuitionisthatwewantthelearningobjectives\nalignment\nofmodelstobealignedwiththegoalsofthehumansthatusethem.",
      "elstobealignedwiththegoalsofthehumansthatusethem.\n\n2 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\n12.1 Prompting\nprompt A prompt is a text string that a user issues to a language model to get the model\nto do something useful. In prompting, the user’s prompt string is passed to the\nlanguagemodel,whichiterativelygeneratestokensconditionedontheprompt.Thus\nthepromptcreatesacontextthatguidesLLMstogenerateusefuloutputstoachieve\nsome user goal.",
      "Mstogenerateusefuloutputstoachieve\nsome user goal. The process of finding effective prompts for a task is known as\nprompt promptengineering.\nengineering\nLet’sseehowtopromptalanguagemodeltosolveasimplesentimentclassifi-\ncationtask,ConsiderthishotelreviewfromtheBLTcorpus(Salvettietal.,2016):\nSampleHotelReview\nDidnotliketheservicethatIwasprovided,whenIenteredthehotel. Ialso\ndid not like the area, in which the hotel was located. Too much noise and\neventsgoingonformetofeelrelax.",
      "Too much noise and\neventsgoingonformetofeelrelax.\nWecangetthemodeltoclassifythesentimentofthistextbytakingthetextand\nappendinganincompletestatementtothereviewlikeIn short, our stay was:\nApromptconsistingofareviewplusanincompletestatement\nDidnotliketheservicethatIwasprovided,whenIenteredthehotel. Ialso\ndid not like the area, in which the hotel was located. Too much noise and\neventsgoingonformetofeelrelax.",
      "Too much noise and\neventsgoingonformetofeelrelax. Inshort,ourstaywas\nWethenhaveanLLMcompletethestatementbygeneratingatokenconditioned\nonthepromptsofar,andthengeneratingthenexttoken(conditionedontheprompt\nplusthenewtoken),andsoforth. Hereareafewresponsesfromalanguagemodel\nviacontinuedgenerationstartingwiththepromptascontext.\nHotelReviewCompletions\nDidnotliketheservicethatIwasprovided,whenIenteredthehotel. Ialso\ndid not like the area, in which the hotel was located.",
      "not like the area, in which the hotel was located. Too much noise and\neventsgoingonformetofeelrelaxed. Inshortourstaywas\n... not a pleasant one. The staff at the front desk\nwere not welcoming or friendly, and seemed disinterested\nin providing good customer service.\n... uncomfortable and not worth the price we paid. We\nwill not be returning to this hotel.\nAs we can see, the overall negative context of the review results in negative\ncompletions.",
      "ext of the review results in negative\ncompletions. We could easily map these completions to the class we are trying\nto predict, perhaps via some predefined mappings, like {excellent → positive},\n{did not like→negative},andsoon.\nThepowerofthisapproachisthatwithsuitableadditionstothecontextasingle\nLLMcanproduceoutputsappropriateformanydifferenttasks. Forexample,given\n\n12.1 • PROMPTING 3\nareviewwemightwantanyofthefollowing:\n• Asummary,\n• Whetherthereviewwastruthfulorlikelytohavebeenfabricated,",
      "thereviewwastruthfulorlikelytohavebeenfabricated,\n• Atranslationtoanotherlanguage.\nLLMshaveastrikingabilitytoperformtaskslikethese,needingjusttheappro-\npriatecontextualnudgetogettheLLMtogeneratethedesiredoutput.\nIf we want to solve general tasks like summarization or translation, we don’t\nwanttohavetocreateanewprompteachtimewedothetask. Insteadthefirststep\ntemplates inpromptingistodesignoneormoretemplates:task-specificpromptingtextalong\nwithslotsfortheparticularinputthatisbeingprocessed.",
      "ithslotsfortheparticularinputthatisbeingprocessed.\nConsiderthefollowingtemplatesforavarietyoftasks:\nBasicPromptTemplates\nSummarization {input};tldr;\nTranslation {input};translate to French:\nSentiment {input};Overall, it was\nFine-Grained- {input};What aspects were important in this review?\nSentiment\nEach template consists of an input text, designated as {input}, followed by a\nverbatimprompttobepassedtoanLLM.",
      "t}, followed by a\nverbatimprompttobepassedtoanLLM.Thesetemplatesareappliedtoinputsto\ncreate filled prompts – instantiated prompts suitable for use as inputs to an LLM.\nFig.12.1illustratesfilledpromptsforthesetemplatesusingourearlierhotelreview,\nalongwithsampleoutputsfromanLLM:\nNotice the design pattern of the prompts above: the input is followed by some\ntext which in turn will be completed by the desired response.",
      "in turn will be completed by the desired response. This style, with the\ninstruction at the end, is common in prompting because it helpfully constrains the\ngeneration. Consider,bycontrast,thepromptinExample12.1.\nTranslateEnglishtoFrench:\nDidnotliketheservicethatIwasprovided! (12.1)\nThis prompt doesn’t do a good job of constraining possible continuations. Instead\nofaFrenchtranslation,modelsgiventhispromptmayinsteadgenerateanothersen-\ntenceinEnglishthatsimplyextendstheEnglishreview.",
      "-\ntenceinEnglishthatsimplyextendstheEnglishreview.Promptsneedtobedesigned\nunambiguously, so that any reasonable continuation would accomplish the desired\ntask(ReynoldsandMcDonell,2021).\nAn even more constraining style of prompt can specify the set of possible an-\nswersintheprompt.Forexamplehereisaprompttemplatetodosentimentanalysis\nthatprespecifiesthepotentialanswers:\nApromptconsistingofareviewplusanincompletestatement\nHuman: Doyouthinkthat“input”hasnegativeorpositivesentiment?",
      "youthinkthat“input”hasnegativeorpositivesentiment?\nChoices:\n(P)Positive\n(N)Negative\nAssistant: Ibelievethebestansweris: (\n\n4 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\nLLMOutputsforBasicPrompts\nOriginalReview($INPUT) Did not like the service that I was provided,\nwhen I entered the hotel. I also did not like\nthe area, in which the hotel was located. Too\nmuch noise and events going on for me to feel\nrelax and away from the city life.",
      "for me to feel\nrelax and away from the city life.\nSentiment Prompt: $INPUT + In short, our stay was\nOutput: not enjoyable\nFine-grainedSentiment Prompt: $INPUT + These aspects were important to\nthe reviewer:\nOutput: 1. Poor service 2. Unpleasant location\n3. Noisy and busy area\nSummarization Prompt: $INPUT + tl;dr\nOutput: I had a bad experience with the hotel’s\nservice and the location was loud and busy.",
      "otel’s\nservice and the location was loud and busy.\nTranslation Prompt: $INPUT + Translate this to French\nOutput: Je n’ai pas aim´e le service qui m’a ´et´e\noffert lorsque je suis entr´e dans l’hˆotel. Je\nn’ai ´egalement pas aim´e la zone dans laquelle se\ntrouvait l’hˆotel. Trop de bruit et d’´ev´enements\npour que je me sente d´etendu et loin de la vie\ncitadine.\nFigure12.1 LLMoutputsforsimplepromptsforsentiment,summarizationandtranslationforaninputtext.",
      "ntiment,summarizationandtranslationforaninputtext.\nThis promptuses anumber of moresophisticated promptingcharacteristics. It\nspecifiesthetwoallowablechoices(P)and(N),andendsthepromptwiththeopen\nparenthesis that strongly suggests the answer will be (P) or (N). Note that it also\nspecifiestheroleofthelanguagemodelasanassistant.\nWe can do even more with prompts. For example, we might want to restrict a\nsummarytobeaparticularlength, tohaveananswergeneratedaccordingtosome",
      "larlength, tohaveananswergeneratedaccordingtosome\nkindofpersonaorrole,ortospecifyamorestructuredoutputusingaprogramming\nlanguage or a data interchange format such as JSON. Or we may want to prompt\nthesystemtobreakdowncomplextasks,usingmethodslikechain-of-thoughtthat\nwe’ll discuss in Section 12.4. All of these kinds of instructions go beyond simple\npromptingandrequirefurtherLLMfinetuningtoenablethemtofollowinstructions.\nWe’llreturntothisnotionofinstructiontuninginSection12.3.",
      "eturntothisnotionofinstructiontuninginSection12.3.\nIn summary, we prompt an LM by transforming each task into a form that is\namenabletocontextualgenerationbyanLLM,asfollows:\n1. Foragiventask,developaatask-specifictemplatethathasafreeparameter\nfortheinputtext.\ntemplate 2. Giventhatinputandthetask-specifictemplate,theinputisusedtoinstantiate\nafilledpromptthatisthenpassedtoapretrainedlanguagemodel.\n3. Autoregressivedecodingisthenusedtogenerateasequenceoftokenoutputs.\n4.",
      "ingisthenusedtogenerateasequenceoftokenoutputs.\n4. Theoutputofthemodelcaneitherbeuseddirectlyasthedesiredoutput(as\ninthecaseofnaturallygenerativetaskssuchastranslationorsummarization),\noratask-appropriateanswercanbeextractedfromthegeneratedoutput(asin\nthecaseofclassification).\n\n12.1 • PROMPTING 5\n12.1.1 LearningfromDemonstrations: Few-ShotPrompting\nIt’softenpossibletoimproveapromptbyincludingsomelabeledexamplesinthe\ndemonstrations prompt template. We call such examples demonstrations.",
      "pt template. We call such examples demonstrations. The task of prompting\nfew-shot with examples is sometimes called few-shot prompting, as contrasted with zero-\nzero-shot shotpromptingwhichmeansinstructionsthatdon’tincludelabeledexamples.\nFig. 12.2 illustrates a few-shot example from an extractive question answering\ntask. Thecontextcombinesthetaskdefinitionalongwiththreegold-standardques-\ntionandanswerpairsfromthetrainingset.",
      "tandardques-\ntionandanswerpairsfromthetrainingset.\nDefinition: This task is about writing a correct answer for the reading comprehension task.\nBased on the information provided in a given passage, you should identify the shortest\ncontinuous text span from the passage that serves as an answer to the given question. Avoid\nanswersthatareincorrectorprovidesincompletejustificationforthequestion.\nPassage: Beyonce´ Giselle Knowles-Carter (born September 4, 1981) is an American singer,",
      "r (born September 4, 1981) is an American singer,\nsongwriter,recordproducerandactress. BornandraisedinHouston,Texas,sheperformedin\nvarioussinginganddancingcompetitionsasachild,androsetofameinthelate1990saslead\nsingerofR&Bgirl-groupDestiny’sChild. Managedbyherfather,MathewKnowles,thegroup\nbecame one of the world’s best-selling girl groups of all time. Their hiatus saw the release\nofBeyonce´’sdebutalbum,DangerouslyinLove(2003),whichestablishedherasasoloartist",
      "uslyinLove(2003),whichestablishedherasasoloartist\nworldwide,earnedfiveGrammyAwardsandfeaturedtheBillboardHot100number-onesingles\n“CrazyinLove”and“BabyBoy”.\nExamples:\nQ:InwhatcityandstatedidBeyonce´ growup?\nA:Houston,Texas\nQ:WhatareasdidBeyonce´ competeinwhenshewasgrowingup?\nA:singinganddancing\nQ:WhendidBeyonce´ releaseDangerouslyinLove?\nA:2003\nQ:WhendidBeyonce´ startbecomingpopular?\nA:\nFigure12.2 A prompt for extractive question answering, from an example from the SQuAD 2.",
      "stion answering, from an example from the SQuAD 2.0 dataset\n(Rajpurkar et al., 2018). The prompt contains the task definition, the passage, 3 demonstration examples,\nfollowedbythetestquestion. ThisdefinitionspecificationandformatareaftertheNaturalInstructionsdataset\n(Mishraetal.,2022).\nHowManyDemonstrations? Thenumberofdemonstrationsdoesn’thavetobe\nlarge. A small number of randomly selected labeled examples used as demonstra-",
      "omly selected labeled examples used as demonstra-\ntions can be sufficient to improve performance over the zero-shot setting. Indeed,\nthe largest performance gains in few-shot prompting tends to come from the first\ntraining example, with diminishing returns for subsequent demonstrations. This is\nincontrastwithfinetuningofspecializedclassifierheadsthatwesawinChapter11\nwhereithelpstohavelotsofexamples.",
      "wesawinChapter11\nwhereithelpstohavelotsofexamples.\nWhyisn’titusefultohavemoredemonstrations? Thereasonisthattheprimary\nbenefitinexamplesistodemonstratethetasktobeperformedtotheLLMandthe\nformat of the sequence, not to provide relevant information as to the right answer\n\n6 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\nforanyparticularquestion. Infact,demonstrationsthathaveincorrectanswerscan\nstill improve a system (Min et al., 2022; Webson and Pavlick, 2022).",
      "stem (Min et al., 2022; Webson and Pavlick, 2022). Adding too\nmanyexamplesseemstocausethemodeltooverfittodetailsoftheexactexamples\nchosenandgeneralizepoorly.\nHowtoSelectDemonstrations? Demonstrationsaregenerallycreatedbyformat-\nting examples drawn from a labeled training set. There are some heuristics about\nwhatmakesagooddemonstration. Forexample,usingdemonstrationsthataresim-\nilar to the current input seems to improve performance.",
      "to the current input seems to improve performance. It can thus be useful to\ndynamicallyretrievedemonstrationsforeachinput,basedontheirsimilaritytothe\ncurrent example (for example, comparing the embedding of the current example\nwithembeddingsofeachofthetrainingsetexampletofindthebesttop-T).\nButmoregenerally,thebestwaytoselectdemonstrationsfromthetrainingset\nis programmatically: choosing the set of demonstrations that most increases task\nperformance of the prompt on a test set.",
      "ases task\nperformance of the prompt on a test set. Task performance for sentiment analysis\nor multiple-choice question answering can be measured in accuracy; for machine\ntranslationwithchrF,andforsummarizationviaRouge. SystemslikeDSPy(Khat-\ntabetal.,2024), aframeworkforalgorithmicallyoptimizingLMprompts, canau-\ntomaticallyfindtheoptimumsetofdemonstrationstoincludebysearchingthrough\nthe space of possible demonstrations to include. We’ll return to automatic prompt\noptimizationinSection12.5.\n12.1.",
      "automatic prompt\noptimizationinSection12.5.\n12.1.2 In-ContextLearningandInductionHeads\nAsawayofgettingamodeltodowhatwewant,promptingisfundamentallydiffer-\nentthanpretraining. Learningviapretrainingmeansupdatingthemodel’sparame-\ntersbyusinggradientdescentaccordingtosomelossfunction. Butpromptingwith\ndemonstrationscanteachamodeltodoanewtask.Themodelislearningsomething\nasitprocessestheprompt.\nEvenwithoutdemonstrations,wecanthinkoftheprocessofpromptingasakind\nof learning.",
      "anthinkoftheprocessofpromptingasakind\nof learning. For example, the further a model gets in a prompt, the better it tends\ntogetatpredictingtheupcomingtokens. Theinformationinthecontextishelping\ngivethemodelmorepredictivepower.\nin-context Thetermin-contextlearningwasfirstproposedbyBrownetal.(2020)intheir\nlearning\nintroductionoftheGPT3system,torefertoeitherofthesekindsoflearningthatlan-\nguage models do from their prompts.",
      "arningthatlan-\nguage models do from their prompts. In-context learning means language models\nlearning to do new tasks, better predict tokens, or generally reduce their loss dur-\ning the forward-pass at inference-time, without any gradient-based updates to the\nmodel’sparameters.\nHow does in-context learning work? While we don’t know for sure, there are\ninductionheads some intriguing ideas. One hypothesis is based on the idea of induction heads\n(Elhageetal.,2021;Olssonetal.,2022).",
      "duction heads\n(Elhageetal.,2021;Olssonetal.,2022). Inductionheadsarethenameforacircuit,\nwhich is a kind of abstract component of a network. The induction head circuit\nis part of the attention computation in transformers, discovered by looking at mini\nlanguagemodelswithonly1-2attentionheads.\nThefunctionoftheinductionheadistopredictrepeatedsequences.Forexample\nif it sees the pattern AB...A in an input sequence, it predicts that B will follow,\ninstantiatingthepatterncompletionruleAB...A→B.",
      "ow,\ninstantiatingthepatterncompletionruleAB...A→B.Itdoesthisbyhavingaprefix\nmatchingcomponentoftheattentioncomputationthat,whenlookingatthecurrent\ntokenA,searchesbackoverthecontexttofindapriorinstanceofA.Ifitfindsone,\ntheinductionheadhasacopyingmechanismthat“copies”thetokenBthatfollowed\n\n12.2 • POST-TRAININGANDMODELALIGNMENT 7\ntheearlierA,byincreasingtheprobabilitytheBwilloccurnext. Fig.12.3showsan\nexample.\nFiFgiguurree11:2In.3theseAquneninced“u.c..vtiinotnagheecaards.l.o.",
      "In.3theseAquneninced“u.c..vtiinotnagheecaards.l.o.voikntiangge”a,atn v i i nd n u t c a tio g n e heuasdeisdetnhteifiepsrtehfiexinmitiaaltocchciunrrgenmceeocfh“avninitsamge”t,o\nfinadtteandpsrtiootrheinsusbtsaenqcueenotwfovridn“ctaarsg”efo,rapnredfixthmeatccohipnyg,inangdmpreedcihctasn“icsamrs”taostphreendexicttwtohrdatthcroaurghstwheiclolpoycincgur\nagmaeinch.aFniisgmu.refromCrosbieandShutova(2022).\ndetOermlsisnoesneeacthahl.ea(d2’0s2in2d)eppernodpenotsoeuttphuattfoartgheener4a.",
      "2’0s2in2d)eppernodpenotsoeuttphuattfoartgheener4a.l2izeIddefnutizfyziyngvIenrdsuioctnioonfHtehaidsspatterncom-\npl c e u t r i r o en n t r to u k l e e n , . implementingarulelikeA*B*.T.o.idAe→ntifBy,inwduhcetiroenhAea*ds≈wAithiannmdodBe*ls,≈weBme(ab-y\n≈w L e ev m era e g a i n ng th th e is y d t e h c e o y mp a o r s e iti s o e n m , E a l n h t a i g c e al e l t y a s l. imisluarretihnesaobimliteyowfaayll)a,ttmenitgiohnthbeeadrsetsoppoenrfsoirbmle",
      "faayll)a,ttmenitgiohnthbeeadrsetsoppoenrfsoirbmle\n(2021)discoveredadistinctbehaviourincertain prefixmatchingonrandominputsequences.4We\nfoartteinnt-icononhetaedxst,wlehaicrhntihnegy.naSmuegdginedsutcitvioenehveaiddse.nce\nfol\nf\nl\no\now\nr\nt\nt\nh\nh\ne\ne\nt\ni\na\nr\nsk\nh\n-a\ny\ng\np\nn\no\nos\nt\nt\nh\nic\ne\na\ns\np\ni\np\ns\nro\nc\na\no\nc\nm\nhto\nes\nco\nf\nm\nro\npu\nm\ntin\nC\ngp\nr\nr\no\ne\ns\n-\n-\nablating biTehaisnbdehSavhiouutroevmae(rg2e0s2w2h)e,nwthhesoehsehaodwsprtohcaetssablfiaxtimnagtchinindgusccotrieosnouhtleinaeddsbycaBuansseasle",
      "gtchinindgusccotrieosnouhtleinaeddsbycaBuansseasleitna-l.c(o20n2t3e)x.t\nlesaerqnuienngcespeofrftohermfoarmnc\"e[At]o[Bd]ec..r.e[aAs]e→. A\".bIlnatioWneiasrgoureigthiantafollcyusaingmseoldeilycaolntperermfixmmaetcahniningg\nth t e he r s e e m he o a v d a s, l th o e f Q so K m ci e r t c h ui i t n d g i . rec W ts e att u en s t e io i n t t i o n - NsLcoPreisnitsesrupfrfiectiaenbtilfiotryousrtuadnaileyssisa,sasahtiogholprfeo-r",
      "btilfiotryousrtuadnaileyssisa,sasahtiogholprfeo-r\nwards[B],whichappearsdirectlyaftertheprevious fixmatchingcoresspecificallyindicateinduction\nteoscticnurgrenccaeuosfathleecfufrerecnttst;okiefnw[Ae].kTnhoiscbkehoavuiotuar hy\nh\np\ne\no\nad\nt\ns\nh\n,\ne\nw\ns\nh\ni\ni\nz\nle\ned\nles\nc\ns\na\nre\nu\nl\ns\nev\ne\na\n,\nn\nw\nth\ne\nea\nw\nds\no\nte\nu\nn\nl\nd\nd\nto\ne\ns\nx\nh\np\no\ne\nw\nc\nh\nt\nig\nth\nh\ne\nefifsetcetrmtoeddpirseafipxpmeaatcr.hiCngr.oTsbheieOaVncdircSuhitustuobvsea-(20co2p2y)inagbclaaptaebiilnitideusc(Btiaonnsalheetaadl.",
      "p2y)inagbclaaptaebiilnitideusc(Btiaonnsalheetaadl.,s2b02y3fi).rWstefigennd--\ninqgueantttleyninticorenasheseathdesouthtpauttploegritfoofrmthea[Bs]intodkuenc,tionerahteeaadsseqounenrcaenodfo50mrainndpoumttsoekqenuse,nexccelusd,ianngd\nth t e e n rm z ed er c o o i p n yi g ng o . u A t n t o h v e er o vi u ew tp o u f t th o i f sm th e e ch s a e ni h sm ea i d s sbtyhese4%ttinmgosctecrotmaminontearnmdsleoasfttchoemomuotnptuotkemnsa.-\nshowninFigure1.",
      "dsleoasfttchoemomuotnptuotkemnsa.-\nshowninFigure1. Thissequenceisrepeatedfourtimestoformthe\ntrixWO tozero. Indeedtheyfindthatablatedmodelsaremuchworseatin-context\ninputtothemodel.Theprefixmatchingscoreiscal-\nle4arnMinegt:htohdesyhavemuchworseperformancceulaatteldebayrnavienrgagifnrgotmhedatetemntoionnsvtarlauteisofnrosmineacthhe\nprompts. tokentothetokensthatdirectlyfollowedthesame\n4.1 Models\ntokeninearlierrepeats.",
      "yfollowedthesame\n4.1 Models\ntokeninearlierrepeats.Thefinalprefixmatching\nWe utilise two recently developed open-source scoresareaveragedoverfiverandomsequences.\n12.2 Pos m t- od t e r ls a ,n i am n el i y n Ll g ama a -3- n 8B d 2an M dInt o ern d LM e 2 l -20 A B ligTnhemprefiexnmattchingscoresforLlama-3-8Bare\n(Caietal.,2024),bothofwhicharebasedonthe showninFigure2.ForIntermLM2-20B,werefer\noriginalLlama(Touvronetal.,2023a)architec- toFigure8inAppendixA.1.Bothmodelsexhibit\nture.",
      "ec- toFigure8inAppendixA.1.Bothmodelsexhibit\nture. Thesemodelsfeaturegrouped-queryatten- headswithnotablyhighprefixmatchingscores,\ntionmechanisms(Ainslieetal.,2023)toenhance distributedacrossvariouslayers.IntheLlama-3-\nWith simple prompting, LLMs have been successfully applied to a range of appli-\nefficiency.Llama-3-8B,comprises32layers,each 8Bmodel,~3%oftheheadshaveaprefixmatching\ncations without the need to update the parameters in the underlying models.",
      "to update the parameters in the underlying models. Nev-\nwith32attentionheadsanditusesaquerygroup scoreof0.3orhigher,indicatingadegreeofspe-\nerstihzeeloefs4s,atttheentrieonahreeadlism. Iittshatsoshhoowwnsmupuecrihorcanciableisaetixonpeincptreedfixfmroamtchiangm,anoddseolmwehheoadssehsavoele\ntrapienrfionrmgaonbcejeccotmivpeariesdttoopitsrepdreidcetctehsseornse,xevtewnordhifgrhosmcorleasrogfeupamtoo0u.9n8.tsofpretrainingtext.\nthelargerLlama-2models.",
      "u.9n8.tsofpretrainingtext.\nthelargerLlama-2models.\nTo see this, consider the following failed examples of following instructions from\nInternLM2-20B,featuring48layerswith48at- 4.3 HeadAblations\nearlyworkwithGPT(Ouyangetal.,2022).\ntentionheadseach,usesaquerygroupsizeof6 Toinvestigatethesignificanceofinductionheads\nattentionheads. WeselectedInternLM2-20Bfor foraspecificICLtask,weconductzero-ablations",
      "0Bfor foraspecificICLtask,weconductzero-ablations\nitsPexreommplparty:pEerxfoprlmaiannctehoenmthoeoNneeldalne-dinin-tghet-oaosfi1x%yaenadr3o%ldofitnheahefaedwswsiethnttheenhciegsh.estprefix\nHaOystuactkp 3 utta:skE,wxphilcahinastshesesetshLeoLMrys’oafbgilritayvtiotytomaatc6hiyngeascroorelsd..Thisablationprocessinvolves\nretrieveasinglecriticalpieceofinformationem- maskingthecorrespondingpartitionoftheoutput\nbedded within a lengthy text. This mirrors the matrix,denotedasWhinEq.",
      "thy text. This mirrors the matrix,denotedasWhinEq. 1,bysettingitto\no\nfunPctrioonmaliptyt:ofTirnadnusctliaotnehteoadFs,rewnhcichh:sTcahnethsemalzledroo.gThiseffectivelyrenderstheheadsinactive\nconOteuxttfpourptr:ioTrhoeccsumrreanlclesdoofgactorkoesnsteodexthtraectroad.\nrelevantsubsequentinformation.\n4Inthiswork,theterm\"inductionheads\"referstowhat\nwedefineasbehaviouralinductionheads,notmechanistic\nones.Atrueinductionheadmustbeverifiedmechanistically;\nH2htetrpes:,/t/haie.mLetLa.",
      "rifiedmechanistically;\nH2htetrpes:,/t/haie.mLetLa.Mcomi/gblnoog/rmeesta-thlleamian-3te/ntofthehorweeqvuere,osutraannadlysriseelmiepsloyisnpsretefixa-dmaotcnhinigtssconreastausraal\n3https://github.com/gkamradt/LLMTest_ proxy.Wewillcontinuetousetheterm\"inductionheads\"for\ninNceleidnleaItniAoHnaystotacakutoregressivelygenerateconstiimnpuliacittyiothnrosugchoounttshiesrteestnotftwheiptahperi.tscontext. In\nthefirstexample,itoutputsatextsomewhatsimilartotheoriginalrequest,andinthe",
      "atextsomewhatsimilartotheoriginalrequest,andinthe\nseconditprovidesacontinuationtothegiv4eninput,ignoringtherequesttotranslate.\nLLMsarenotsufficientlyhelpful: theyneedextratrainingtoincreasetheirabilities\ntofollowtextualinstructions.\nAdeeperproblemisthatLLMscansimultaneouslybetooharmful. Pretrained\nlanguage models easily generate text that is harmful in many ways. For example\n\n8 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING",
      "• MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\ntheycangeneratetextthatisfalse,includingunsafemisinformationlikegivingdan-\ngerouslyincorrectanswerstomedicalquestions. Andtheycangeneratetextthatis\ntoxic in many ways, such as facilitating the spread of hate speech. Gehman et al.\n(2020)showthatevencompletelynon-toxicpromptscanleadlargelanguagemod-\nels to output hate speech and abuse their users. Or language models can generate\nstereotypes (Cheng et al., 2023) and negative attitudes (Brown et al.",
      "et al., 2023) and negative attitudes (Brown et al., 2020; Sheng\netal.,2019)aboutmanydemographicgroups.\nOne reason LLMs are too harmful and insufficiently helpful is that their pre-\ntrainingobjective(successatpredictingwordsintext)ismisalignedwiththehuman\nneedformodelstobehelpfulandnon-harmful.\nInanattempttoaddressthesetwoproblems,languagemodelsgenerallyinclude\nmodel twoadditionalkindsoftrainingformodelalignment: methodsdesignedtoadjust\nalignment",
      "modelalignment: methodsdesignedtoadjust\nalignment\nLLMstobetteralignthemtohumanneedsformodelstobehelpfulandnon-harmful.\nInthefirsttechnique,instructiontuning(orsometimescalledSFTforsupervised\nfinetuning), models are finetuned on a corpus of instructions and questions with\ntheircorrespondingresponses. Inthesecondtechnique,preferencealignment,of-\nten called RLHF after one of the specific instantiations, Reinforcement Learning\nfromHumanFeedback,aseparatemodelistrainedtodecidehowmuchacandidate",
      ",aseparatemodelistrainedtodecidehowmuchacandidate\nresponse aligns with human preferences. This model is then used to finetune the\nbasemodel.\nbasemodel We’ll use the term base model to mean a model that has been pretrained but\naligned hasn’tyetbeenalignedeitherbyinstructiontuningorRLHF.Andwerefertothese\npost-training stepsaspost-training,meaningthattheyapplyafterthemodelhasbeenpretrained.\n12.3 Model Alignment: Instruction Tuning",
      "trained.\n12.3 Model Alignment: Instruction Tuning\nInstruction Instruction tuning (short for instruction finetuning, and sometimes even short-\ntuning\nenedtoinstructtuning)isamethodformakinganLLMbetteratfollowinginstruc-\ntions. ItinvolvestakingabasepretrainedLLMandtrainingittofollowinstructions\nforarangeoftasks,frommachinetranslationtomealplanning,byfinetuningiton\na corpus of instructions and responses.",
      "tuningiton\na corpus of instructions and responses. The resulting model not only learns those\ntasks,butalsoengagesinaformofmeta-learning–itimprovesitsabilitytofollow\ninstructionsgenerally.\nInstructiontuningisaformofsupervisedlearningwherethetrainingdatacon-\nsists of instructions and we continue training the model on them using the same\nlanguagemodelingobjectiveusedtotraintheoriginalmodel. Inthecaseofcausal\nmodels,thisisjustthestandardguess-the-next-tokenobjective.",
      "hisisjustthestandardguess-the-next-tokenobjective. Thetrainingcorpus\nof instructions is simply treated as additional training data, and the gradient-based\nupdates are generated using cross-entropy loss as in the original model training.\nEventhoughitistrainedtopredictthenexttoken(whichwetraditionallythinkof\nSFT as self-supervised), we call this method supervised fine tuning (or SFT) because\nunlikeinpretraining,eachinstructionorquestionintheinstructiontuningdatahas",
      "nstructionorquestionintheinstructiontuningdatahas\nasupervisedobjective: acorrectanswertothequestionoraresponsetotheinstruc-\ntion.\nHowdoesinstructiontuningdifferfromtheotherkindsoffinetuningintroduced\ninChapter10andChapter11? Fig.12.4sketchesthedifferences. Inthefirstexam-\nple,introducedin,Chapter10wecanfinetuneasawayofadaptingtoanewdomain\nbyjustcontinuingpretrainingtheLLMondatafromanewdomain.Inthismethod\nalltheparametersoftheLLMareupdated.",
      ".Inthismethod\nalltheparametersoftheLLMareupdated.\n\n12.3 • MODELALIGNMENT: INSTRUCTIONTUNING 9\nPretraining Finetuning\nInference\nNext word\nData from\nPretrained LLM finetuning prediction\ndomain objective\nContinue\nFinetuning as … p tr a a r i a n m in e g t e a r l s l … On finetuning\nContinued on finetuning domain\nPretraining domain\nNext word\nData from\nprediction\nfinetuning +\nPretrained LLM domain objective\nParameter\nTrain only new\nEfficient … parameters on … A On finetuning",
      "y new\nEfficient … parameters on … A On finetuning\nFinetuning finetuning B domain\ndomain\n(e.g., LoRA)\nSupervised Task\ndata from specific\ntask loss\nPretrained LLM\nTrain only\nclassification\n… On finetuning\nMLM … head on task\nfinetuning\nFinetuning\ntask\nSupervised\ninstructions Next word\nprediction\nInstruction objective\nInstruction\nTuning … tuning on … On unseen\ntasks\n(SFT) diverse\ntasks\nFigure12.4 Instructiontuningcomparedtotheotherkindsoffinetuning.",
      "tructiontuningcomparedtotheotherkindsoffinetuning.\nInthesecondexample, alsofromChapter10, parameter-efficientfinetuning,\nweadapttoanewdomainbycreatingsomenew(small)parameters,andjustadapt-\ningthemtothenewdomain. InLoRA,forexample,it’stheAandBmatricesthat\nweadapt,butthepretrainedmodelparametersarefrozen.\nIn the task-based finetuning of Chapter 11, we adapt to a particular task by\nadding a new specialized classification head and updating its features via its own\nloss function (e.g.",
      "ating its features via its own\nloss function (e.g., classification or sequence labeling); the parameters of the pre-\ntrainedmodelmaybefrozenormightbeslightlyupdated.\nFinally, in instruction tuning, we take a dataset of instructions and their super-\nvisedresponsesandcontinuetotrainthelanguagemodelonthisdata,basedonthe\nstandardlanguagemodelloss.\nInstruction tuning, like all of these kinds of finetuning, is much more modest\nthan the training of base LLMs.",
      "s much more modest\nthan the training of base LLMs. Training typically involves several epochs over\ninstruction datasets that number in the thousands. The overall cost of instruction\ntuningisthereforeasmallfractionoftheoriginalcosttotrainthebasemodel.\n12.3.1 InstructionsasTrainingData\nByinstruction,wehaveinmindanaturallanguagedescriptionofatasktobeper-\nformed,combinedwithlabeledtaskdemonstrations. Thiscanincludeminimalde-",
      "beledtaskdemonstrations. Thiscanincludeminimalde-\n\n10 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\nscriptions similar to the prompts we’ve already seen such as Answer the following\nquestion, TranslatethefollowingtexttoArapaho, orSummarizethisreport. How-\never, since we will be using supervised finetuning to update the model, these in-\nstructionsneednotbelimitedtosimplepromptsdesignedtoevokeabehaviorfound\ninthepretrainingcorpora.",
      "gnedtoevokeabehaviorfound\ninthepretrainingcorpora. Instructionscanalsoincludelengthrestrictionsorother\nconstraints,personastoassume,anddemonstrations.\nMany huge instruction tuning datasets have been created, covering many tasks\nand languages. For example Aya gives 503 million instructions in 114 languages\nLang Prompt Completion\nfrom12tasksincludingquestionanswering,summarization,translation,paraphras-\ning, ar s a entim.",
      "rization,translation,paraphras-\ning, ar s a entim.(cid:1688)e(cid:1645)(cid:1712)n(cid:2629)(cid:2061)tﺍ(cid:1888)a(cid:3548)(cid:3763)n(cid:1862)a(cid:1857)lﺡy(cid:1678)s(cid:1857)iﺓs(cid:1678),(cid:1645)(cid:1905)n(cid:1698)aﺀ(cid:1543)t(cid:2800)u(cid:2777)(cid:1542)(cid:4051)r(cid:1854)a(cid:1698)llanguageinferenceand6others(Sً(cid:3080)i(cid:3075)n(cid:1903)َg(cid:1699)h(cid:1722)(cid:1652)(cid:1711)e(cid:1604)ﻙt(cid:2058)a(cid:2042)ُ(cid:2510)(cid:2506)lﺍ.",
      "04)ﻙt(cid:2058)a(cid:2042)ُ(cid:2510)(cid:2506)lﺍ.ﺡ,(cid:1678)2َ(cid:1857)0ﻥﺇ2(cid:1443) 4).\nSuperNatural Instructions has 12 million examples from 1600 tasﺩ(cid:1543)k(cid:3866)ﻭs(cid:2521)(cid:5466)(cid:2513)ﺍَ(ﻭWﻉ(cid:1543)(cid:3871)a(cid:3539)(cid:3522)ﺍn(cid:3843)(cid:3715)g(cid:1678)َ(cid:1857)e(cid:1862)tَ(cid:1564)al.,\n2022), Flan2022has15millionexamplesfrom1836tasks(Long(cid:1854)pُ(cid:4827)(cid:4810)rِ(cid:1857)e(cid:1776)(cid:1699)(cid:3080)e(cid:3075)ِ(cid:2629)t(cid:2061)ﺍa(cid:261",
      "d:3080)e(cid:3075)ِ(cid:2629)t(cid:2061)ﺍa(cid:2611)ّl(cid:1564).(cid:1543),(cid:1614)(cid:1677)َ2(cid:3872)ﺃ0(cid:5466)(cid:1960)َ2(cid:1843) 3),\nandOPT-IMLhas18millionexamplesfrom2000tasks(Iyeretal.,2ﺀ0(cid:1543)(cid:2800)(cid:2777)2(cid:2517)(cid:1443)(cid:2513)2ﺍ(cid:4397))(cid:4383)(cid:2611)َ.َ(cid:2598)ﻱﺯﺍ(cid:1960)ُ(cid:1604)(cid:4693)َ(cid:4648)\nTheseinstruction-tuningdatasetsarecreatedinfourways.Thefirstisforpeople\nfra Qui a écrit le livre La AntonineMailletaécritLaSagouineen1971.",
      "le livre La AntonineMailletaécritLaSagouineen1971.Elleaécritbeau-\ntowritethSaegoinuisntea?ncesdirectly. Focoruepxda’hmistpoilrees,spuarrletsoAfcatdhieensA(lyesafriannsctorpuhcontefisdneeltaucnôtienEgstcor-\nduCanada)etafaitconnaîtrel’Acadieaumonde.\npus(Fig.12.5)includes204Kinstruction/responseinstanceswrittenby3000fluent\nibo Kedu ka ndị Igbo si eme Emumeji ọhụrụn’omenala Igbo, bụemumena-egosinjedebe\nspeakers of 65 languages volunteering as part of a participatory research initiative\nemumejiọhụrụ?",
      "a participatory research initiative\nemumejiọhụrụ? ogeọrụugbo,na-amalitesiten’emumeekelematinyekwaoriri\nwiththegoalofimprovingmultinlianegguwau.lỌpebrụfomrmmemanmceeọgobfakLọLebMeas.na-ekerịtajiọhụrụ,na-\negosipụtaekelenammalitenkeogeọhụrụ,na-emekammekọrịta\nmmadụnaibeyadịkwuoelu.\npor Qualéacapitaldoestadoda AcapitaldoestadodaBahia,noBrasil,éSalvador. Fundada\nBahia? em 1549, foi a primeira capital do Brasil e é conhecida por\nseupatrimônioculturalricoediversificado.",
      "ecida por\nseupatrimônioculturalricoediversificado. Acidadeéfamosa\nporsuaarquiteturacolonial,particularmentenoPelourinho,um\ncentrohistóricoquefoideclaradoPatrimônioMundialpelaUN-\nESCO.\npes ﺮﺗﻮﺒﮐﻪﻧﻻﺪﻨﭼﺪﺒﻴﻣﻪﻧﺎﺧﺮﺗﻮﺒﮐﺝﺮﺑﺭﺩ ﺭﺍﻮﻧﻭﺩﺭﺍﺩﺩﻮﺟﻭﺮﺗﻮﺒﮐﻪﻧﻻﺭﺍﺰﻫ ﺭﺎﻬﭼﻥﺁﻞﺧﺍﺩﻪﮐﺖﺳﺍﯼﺍ ﻪﻧﺍﻮﺘﺳﺍﯽﺟﺮﺑﺪﺒﻴﻣﻪﻧﺎﺧ ﺮﺗﻮﺒﮐ\n؟ﺩﺭﺍﺩﺩﻮﺟﻭ ﺮﻄﺧﺮﺑﺍﺮﺑﺭﺩﺝﺮﺑﻦﻳﺍ .ﺖﺳﺍﻩﺪﺷﻩﺪﻴﺸﮐﺝﺮﺑﺭﻭﺩﺎﺗﺭﻭﺩﻥﺍﺮﺗﻮﺒﮐﺏﺬﺟﯼﺍﺮﺑﯽﮕﻧﺭﺪﻴﻔﺳ\n.ﺖﺳﺍﻩﺩﻮﺑﻦﻤﻳﺍﺭﺎﻴﺴﺑﺎﻫﺮﺗﻮﺒﮐﻪﺑﺕﺎﻧﺍﻮﻴﺣﺮﮕﻳﺩﻪﻠﻤﺣ\nmsa Apakahnasilemak?",
      "ﺭﺎﻴﺴﺑﺎﻫﺮﺗﻮﺒﮐﻪﺑﺕﺎﻧﺍﻮﻴﺣﺮﮕﻳﺩﻪﻠﻤﺣ\nmsa Apakahnasilemak? Nasi lemak merupakan makanan tradisi orang Melayu yang\nterdapat di semua bahagian Malaysia, Singapura, Indonesia\n(terutama di Riau, Jambi serta utara dan pantai timur Su-\nmatera)danBrunei. Sajianinimerujukkepadanasiyangdi-\nmasakdenganmenggunakansantankelapabagimenambahrasa\nlemaknya.Kadangkala,daunpandanwangidimasukkansemasa\nnasidimasakbagimenambahkanaromanya.",
      "masukkansemasa\nnasidimasakbagimenambahkanaromanya.\ntam ெசயற்ைக நுண்ணற(cid:167)வு ெபாதுவாக மனிதர்களால் ெசய்யப்படும் பணிகைளச்\nஎன்றால்என்ன? ெசய்ய ஒரு கணினி அல்லது ஒரு கணினியால்\nகட்டுப்படுத்தப்படும்ஒருேராேபாவ(cid:165)ன்த(cid:166)றன்ெசயற்ைக\nநுண்ணற(cid:167)வுஎனப்படும்.\nFigure12.5 Samplesofprompt/completioninstancesin4ofthe65languagesintheAya\nTable 3: Examples of prompt and completions in the Aya Dataset.\ncorpus(Singhetal.,2024).",
      "tions in the Aya Dataset.\ncorpus(Singhetal.,2024).\nDevelopinghighqualitysupervisedtrainingdatainthiswayistimeconsuming\ntors is not uniform across languages. Moreover, within each language, there is a lack of consistent\nandcostly. Amorecommonapproachmakesuseofthecopiousamountsofsuper-\ncontributionsfromallannotators. Inthissection,weexaminetheimpactofannotatorskewonthe\nvisedtrainingdatathathavebeencuratedovertheyearsforawiderangeofnatural\nresulting dataset.\nlanguage tasks.",
      "erangeofnatural\nresulting dataset.\nlanguage tasks. There are thousands of such datasets available, like the SQuAD\nAnnotatdoartaSsekteowfAqucersotsiosnLsaanngduaangsewse.rAsn(Rnoatjaptuorrksawreerteaeln.,c2ou0r1a6g)edortothceonmtrainbyutdeattoasaentsyolafnguage\nin whichtrtahnesylactoiounldsocromsufmormtaabrliyzarteioadn.aTnhdiswdraittaecaanndbweearuetoamskaetdictaollyfoccounsvmerotestdoinfttohseeirtseoffforts on\nlanguageisnosttrhuecrtitohnanprEonmgpltissahn.",
      "on\nlanguageisnosttrhuecrtitohnanprEonmgpltissahn.dAinltphuotu/oguhtpaustidgenmifiocannsttrantuiomnbpearirosfvpiaartsiicmippalenttsemrepgliastteesr.ed for many\nlanguages, thFeige.n1g2a.g6eimlleunsttraletveesleoxfamanpnloetsaftoorrssowmaesanpoptliecqautiaol,nswfhriocmh rtheseuSltUedPEiRnNcoAnTsUidReAraLbIlNe-differ-\nencesintShTeRnUuCmTbIOerNoSfrceosnoturribcuet(ioWnasnagcreotssalla.,ng2u0a2g2e)s,.sFhiogwuirneg10re(lteovpa)ntprsolovitdsessuacnhoavsertveixetw, ofthe",
      "teovpa)ntprsolovitdsessuacnhoavsertveixetw, ofthe\npercentagceonotfeexatc,hanladnghuyapgoethperseisse.ntToingtehneefirantaelicnosmtrupciltaiotino-ntu.nTinhge hdiagtha,estthensuemfibeelrdsofacnodntthriebutions\nis for Malgargoausnyd-wtriuthth1l4a,b5e9l7sainresteaxntcreasc,teadndfrtohmetlohweetrsatiinsin7g9dfoartaK,uerndciosdhe.daskey/valuepairs,\nand inserted in templates (Fig. 12.7) to produce instantiated instructions. Because\nAnnotatito’sruSskeefuwl fWoritthheinpraomLpatnsgtouabgeed.",
      "to’sruSskeefuwl fWoritthheinpraomLpatnsgtouabgeed.ivTehrseefiinnawl ocordnitnrgib,ultainognusafgoer meaochdellasncgaunagaelsionbtehe Aya\nDataset are not evenly distributed among annotators. The median number of annotators per lan-\nguage is 15 (mean is 24.75) with one language having only a single active annotator (Sindhi) and\n14\n\n12.3 • MODELALIGNMENT: INSTRUCTIONTUNING 11\nusedtogenerateparaphraseoftheprompts.",
      "IONTUNING 11\nusedtogenerateparaphraseoftheprompts.\nFew-ShotLearningforQA\nTask Keys Values\nSentiment text DidnotliketheservicethatIwasprovided...\nlabel 0\ntext Itsoundslikeagreatplot,theactorsarefirstgrade,and...\nlabel 1\nNLI premise NoweaponsofmassdestructionfoundinIraqyet.\nhypothesis WeaponsofmassdestructionfoundinIraq.\nlabel 2\npremise JimmySmith... playedcollegefootballatUniversityofCol-\norado.\nhypothesis TheUniversityofColoradohasacollegefootballteam.",
      "is TheUniversityofColoradohasacollegefootballteam.\nlabel 0\nExtractiveQ/A context Beyonce´ GiselleKnowles-CarterisanAmericansinger...\nquestion WhendidBeyoncestartbecomingpopular?\nanswers {text: [’inthelate1990s’],answer start: 269}\nFigure12.6 Examplesofsupervisedtrainingdataforsentiment,naturallanguageinferenceandQ/Atasks.\nThe various components of the dataset are extracted and stored as key/value pairs to be used in generating\ninstructions.",
      "value pairs to be used in generating\ninstructions.\nTask Templates\nSentiment -{{text}} How does the reviewer feel about the movie?\n-The following movie review expresses what sentiment?\n{{text}}\n-{{text}} Did the reviewer enjoy the movie?\nExtractiveQ/A -{{context}} From the passage, {{question}}\n-Answer the question given the context. Context:\n{{context}} Question: {{question}}\n-Given the following passage {{context}}, answer the\nquestion {{question}}",
      "age {{context}}, answer the\nquestion {{question}}\nNLI -Suppose {{premise}} Can we infer that {{hypothesis}}?\nYes, no, or maybe?\n-{{premise}} Based on the previous passage, is it true\nthat {{hypothesis}}? Yes, no, or maybe?\n-Given {{premise}} Should we assume that {{hypothesis}}\nis true? Yes,no, or maybe?\nFigure12.7 Instructiontemplatesforsentiment,Q/AandNLItasks.\nBecausesupervisedNLPdatasetsarethemselvesoftenproducedbycrowdwork-",
      "NLPdatasetsarethemselvesoftenproducedbycrowdwork-\ners based on carefully written annotation guidelines, a third option is to draw on\nthese guidelines, which can include detailed step-by-step instructions, pitfalls to\navoid,formattinginstructions,lengthlimits,exemplars,etc.Theseannotationguide-\nlinescanbeuseddirectlyaspromptstoalanguagemodeltocreateinstruction-tuning\ntraining examples. Fig. 12.8 shows such a crowdworker annotation guideline that",
      "hows such a crowdworker annotation guideline that\n\n12 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\nwasrepurposedasaprompttoanLLMtogenerateinstruction-tuningdata(Mishra\net al., 2022). This guidelinedescribes a question-answering taskwhere annotators\nprovideananswertoaquestiongivenanextendedpassage.\nSampleExtendedInstruction\n• Definition: Thistaskinvolvescreatinganswerstocomplexquestions,fromagivenpas-\nsage.",
      "tinganswerstocomplexquestions,fromagivenpas-\nsage. Answering these questions, typically involve understanding multiple sentences.\nMakesurethatyouranswerhasthesametypeasthe”answertype”mentionedininput.\nTheprovided”answertype”canbeofanyofthefollowingtypes: ”span”,”date”,”num-\nber”.A”span”answerisacontinuousphrasetakendirectlyfromthepassageorquestion.\nYou can directly copy-paste the text from the passage or the question for span type an-\nswers.",
      "e passage or the question for span type an-\nswers. Ifyoufindmultiplespans,pleaseaddthemallasacommaseparatedlist. Please\nrestricteachspantofivewords. A”number”typeanswercanincludeadigitspecifying\nanactualvalue. For”date”typeanswers,useDDMMYYYYformate.g. 11Jan1992.\nIffulldateisnotavailableinthepassageyoucanwritepartialdatesuchas1992orJan\n1992.\n• Emphasis: If you find multiple spans, please add them all as a comma separated list.\nPleaserestricteachspantofivewords.",
      "separated list.\nPleaserestricteachspantofivewords.\n• Prompt:Writeananswertothegivenquestion,suchthattheanswermatchesthe”answer\ntype”intheinput.\nPassage: {passage}\nQuestion: {question}\nFigure12.8 Example of a human crowdworker instruction from the NATURALINSTRUCTIONS dataset for\nanextractivequestionansweringtask,usedasapromptforalanguagemodeltocreateinstructionfinetuning\nexamples.\nAfinalwaytogenerateinstruction-tuningdatasetsthatisbecomingmorecom-",
      "einstruction-tuningdatasetsthatisbecomingmorecom-\nmon is to use language models to help at each stage. For example Bianchi et al.\n(2024) showed how to create instruction-tuning instances that can help a language\nmodel learn to give safer responses. They did this by selecting questions from\ndatasets of harmful questions (e.g., How do I poison food? or How do I embez-\nzlemoney?). Thentheyusedalanguagemodeltocreatemultipleparaphrasesofthe",
      "sedalanguagemodeltocreatemultipleparaphrasesofthe\nquestions(likeGivemealistofwaystoembezzlemoney),andalsousedalanguage\nmodel to create safe answers to the questions (like I can’t fulfill that request. Em-\nbezzlement is a serious crime that can result in severe legal consequences.). They\nmanuallyreviewedthegeneratedresponsestoconfirmtheirsafetyandappropriate-\nness and then added them to an instruction tuning dataset.",
      "then added them to an instruction tuning dataset. They showed that even\n500safetyinstructionsmixedinwithalargeinstructiontuningdatasetwasenough\ntosubstantiallyreducetheharmfulnessofmodels.\n12.3.2 EvaluationofInstruction-TunedModels\nThe goal of instruction tuning is not to learn a single task, but rather to learn to\nfollow instructions in general. Therefore, in assessing instruction-tuning methods\nweneedtoassesshowwellaninstruction-trainedmodelperformsonnoveltasksfor",
      "aninstruction-trainedmodelperformsonnoveltasksfor\nwhichithasnotbeengivenexplicitinstructions.\nThe standard way to perform such an evaluation is to take a leave-one-out ap-\nproach—instruction-tuneamodelonsomelargesetoftasksandthenassessiton\na withheld task. But the enormous numbers of tasks in instruction-tuning datasets\n\n12.4 • CHAIN-OF-THOUGHTPROMPTING 13\n(e.g.,1600forSuperNaturalInstructions)oftenoverlap;SuperNaturalInstructions\nincludes25separatetextualentailmentdatasets!",
      "tions\nincludes25separatetextualentailmentdatasets! Clearly, testingonawithhelden-\ntailmentdatasetwhileleavingtheremainingonesinthetrainingdatawouldnotbe\natruemeasureofamodel’sperformanceonentailmentasanoveltask.\nToaddressthisissue,largeinstruction-tuningdatasetsarepartitionedintoclus-\ntersbasedontasksimilarity.Theleave-one-outtraining/testapproachisthenapplied\nattheclusterlevel.Thatis,toevaluateamodel’sperformanceonsentimentanalysis,",
      "toevaluateamodel’sperformanceonsentimentanalysis,\nall the sentiment analysis datasets are removed from the training set and reserved\nfor testing. This has the further advantage of allowing the use of a uniform task-\nappropriate metric for the held-out evaluation. SUPERNATURALINSTRUCTIONS\n(Wangetal.,2022),forexamplehas76clusters(tasktypes)overthe1600datasets\nthatmakeupthecollection.\n12.4 Chain-of-Thought Prompting\nThereareawiderangeoftechniquestousepromptstoimprovetheperformanceof",
      "oftechniquestousepromptstoimprovetheperformanceof\nlanguage models on many tasks. Here we describe one of them, called chain-of-\nchain-of- thoughtprompting.\nthought\nThegoalofchain-of-thoughtpromptingistoimproveperformanceondifficult\nreasoning tasks that language models tend to fail on. The intuition is that people\nsolve these tasks by breaking them down into steps, and so we’d like to have lan-\nguage in the prompt that encourages language models to break them down in the\nsameway.",
      "language models to break them down in the\nsameway.\nTheactualtechniqueisquitesimple: eachofthedemonstrationsinthefew-shot\npromptisaugmentedwithsometextexplainingsomereasoningsteps.Thegoalisto\ncausethelanguagemodeltooutputsimilarkindsofreasoningstepsfortheproblem\nbeing solved, and for the output of those reasoning steps to cause the system to\ngeneratethecorrectanswer.\nIndeed, numerousstudieshavefoundthataugmentingthedemonstrationswith",
      "udieshavefoundthataugmentingthedemonstrationswith\nreasoningstepsinthiswaymakeslanguagemodelsmorelikelytogivethecorrect\nanswer difficult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023). Fig. 12.9\nshows an example where the demonstrations are augmented with chain-of-thought\ntextinthedomainofmathwordproblems(fromtheGSM8kdatasetofmathword\nproblems (Cobbe et al., 2021). Fig. 12.10 shows a similar example from the BIG-\nBench-Harddataset(Suzgunetal.,2023).\n12.",
      "the BIG-\nBench-Harddataset(Suzgunetal.,2023).\n12.5 Automatic Prompt Optimization\nGiven a prompt for a task (human or computer generated), prompt optimization\nmethodssearchforpromptswithimprovedperformance. Mostoftheseapproaches\ncanbeviewedasaformofiterativeimprovementsearch(RussellandNorvig,2002)\nthroughaspaceofpossiblepromptsforthosethatoptimizeperformanceonatask.\nAssuch,theseapproachesallsharethefollowingcomponents:\n• A start state – An initial human or machine generated prompt or prompts",
      "tial human or machine generated prompt or prompts\nsuitableforsometask.\n• Ascoringmetric–Amethodforassessinghowwellagivenpromptperforms\n\n14 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\nFigure12.9 Example of the use of chain-of-thought prompting (right) versus standard\nprompting(left)onmathwordproblems.FigurefromWeietal.(2022).\nModel Input (“Answer-Only” Prompting) Model Input (Chain-of-Thought Prompting)",
      "ompting) Model Input (Chain-of-Thought Prompting)\nTask Description T c a o s u k ld d h e a s v c e ri p o t c io c n u : r r A e n d s . wer questions about which times certain events Task Description T c a o s u k ld d h e a s v c e ri p o t c io c n u : r r A e n d s . wer questions about which times certain events\nQ: Today, Tiffany went to the beach. Between what times could they\nQ: Today, Tiffany went to the beach.",
      "es could they\nQ: Today, Tiffany went to the beach. Between what times could they Question have gone? We know that:\nQuestion have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]\nOptions T O i p ffa ti n o y n s w : o ke ( ( A C u ) ) p 9 5 a a a t m m 5 a t t o o m 1 6 . 2 a [. p m .. m ] T he be ( ( a B D c ) ) h 1 3 w 2 p p a m m s t c o t l o o 4 s 2 p e p m d m a fter 4pm. [...",
      "s t c o t l o o 4 s 2 p e p m d m a fter 4pm. [...] Options Options: ( ( A C ) ) 9 5 a a m m t t o o 1 6 2 a p m m ( ( B D ) ) 1 3 2 p p m m t o to 4 2 p p m m\nA: Let's think step by step.\nAnswer A: (D) Chain-of-Thought W the a k b e e - a u c p h t i w m a e s : 5 3 a p m m . t [ o .. . 4 ] p T m he . S o o n l t y h e tim a e n s w w h e e r n is T ( if D fa ). ny could have gone to\nT Q es u t e - s T t i i m on e Q t H O h a : p e n t T y i n o o h a d n a h a s v y : e w , o H g k ( ( o a A",
      "o o h a d n a h a s v y : e w , o H g k ( ( o a A C e n n ) ) e n u 3 5 ? a p p p h W a m m w t e 5 t t e o o k a n n m 5 6 t o p p t . o w m m [ . t . t h . h ] e a T t s h : o e c s c ( ( o e B D c r ) ) c f i 1 e 1 e 1 r p l d a m fi . m e B l t d o t e o t w 3 w 1 p a e p m s e m c n l o w s h e a d t a tim fte e r s 6 c p o m u . l d [. ..] T Q es u t e - s T t i i m on e Q t H O h a : p e n t T y i n o o h a d n a h a s v y : e w , o H g k ( o a A e n n ) e n u 3 ?",
      "a s v y : e w , o H g k ( o a A e n n ) e n u 3 ? a p p h W a m w t e 5 t e o k a n n m 5 t o p t . o w m [ . t . t h . h ] e a T t s h : o e c s c ( o e B c r ) c f i 1 e e 1 r l d a fi . m e B l d t e o t w w 1 a e p s e m c n l o w s h e a d t a tim fte e r s 6 c p o m u . l d [. ..]\n(C) 5pm to 6pm (D) 1pm to 3pm\nA:\nA: Let's think step by step.\nModel Output Model Output\nGen A e n r s a w te e d r (B) W 5a a m k - e 6 - a u m p : t im bu e y : i n 5 g a m cl . o thes at the mall.",
      "t im bu e y : i n 5 g a m cl . o thes at the mall.\n6am-11am: watching a movie at the theater.\n11am-1pm: getting a coffee at the cafe.\nGenerated 1pm-3pm: working at the office.\nChain-of-Thought 3pm-5pm: waiting at the airport.\n5pm-6pm: free. The soccer field closure time: 6pm.\nThe only time when Hannah could have gone to the soccer field was\n5pm to 6pm. So the answer is (C).\nFFiigguurree31:2A.1n0illusEtrxaatimonpoleftohefttwhoepurosempotfincghsaeitnup-osfw-teheoxupglohrtepinroomurpptainpegr((arni",
      "tnup-osfw-teheoxupglohrtepinroomurpptainpegr((arnigswhte)r-vosnlystaannddCarodTpprroommpptitningg).B(loetfht)seitnupas\nrienacsloudneintagsktadsekscornipttieomnspaonrdalospetiqounsenincitnheg.inFpiugtuprreomfrpot.mThSeutzagskunheerteaisl.T(e2m0p2o3r)a.lSequences.\n“let’sthinkstep-by-step”(Kojimaetal.,2022)to dardinmanypriorwork(Brownetal.,2020;Rae\nallCoTannotationsintohnetfheewt-ashsko.texemplars.An etal.,2021;Hoffmannetal.,2022;Srivastavaetal.",
      "s.An etal.,2021;Hoffmannetal.,2022;Srivastavaetal.,\nexampleofaCoTpr•omApntiesxsphaonwsinoinnmFiegtuhroed3–. Ame2th0o2d2)f,oirtgteynpeicraaltliynguvnadreiraetsiotinmsaotefsamproodmelptp.erfor-\nLanguage models. We consider three fami- manceonchallengingtasks,suchasthosethatre-\nlies of language\nGi\nm\nve\no\nn\nde\nth\nls\ne\n:\nen\nC\no\no\nr\nd\nm\ne\no\nx\nus\n(C\nv\nh\na\ne\nri\nn\nat\ne\nio\nt\nn\na\ni\nl\nn\n.,\nhowqupirreommputlstipfolerraesaisnognliengtasstkepcsa.",
      "upirreommputlstipfolerraesaisnognliengtasstkepcsa.nIbnethexepsreetstisnegdrien-\n2021a),InstructG\nla\nP\nn\nT\ngu\n(\na\nO\ng\nu\ne\ny\n,\na\ns\nn\ne\ng\nar\ne\nc\nt\nh\na\nm\nl.,\ne\n2\nth\n02\no\n2\nd\n;\ns\nB\nha\nro\nv\nw\ne\nn\ntobepcoorntesdtrianin(Sedrivtoasatarveaaesotnala.,b2le02sp2)a,cneo.nBeeoafmthseemarocdh-\netal.,2020),andisPaaLwMid(eClyhouwseddhemryetehtoadl.,th2a0t22co).mbienless(ibnrcelauddtihn-gfiPrsatLsMea5rc4h0Bw)iothutapefirxfoerdm-wedidhtuhmparin--",
      "Mea5rc4h0Bw)iothutapefirxfoerdm-wedidhtuhmparin--\nForCodex,wefoorcituysqounecuoedteh-adtafvoincucis-e0s0t2h,ecsoedaer-cherfaftoerrtboansetlhineetsoopnpaenryfoorfmthiengtasvkasrimanetest.inFgigth.e1B2B.1H1\ndavinci-002,andocuotldien-ecsutshhemgaenn-0e0ra1l.aFpoprrIonasctrhubcte-hindcrmiteorsita.cuTrhreenftepwr-osmhoptteovpatliumaitzioantioonfPmaeLthMod5s4.0B\nGPT,weusetext-davBinegcii-n0n0i2n,gtewxitt-hcuirniiet-i0a0l2c,atnedxitd-atewpirtohmanpst(wse),r-tohnelyalpgroormitphtminggeinnethraistepsavpa",
      "e),r-tohnelyalpgroormitphtminggeinnethraistepsavpaerr,iahnotwseavnedr,\nbabbgage-001, aandddstetxhte-amdato-0a01li.stFoofrpPraoLmMp,tswteobeocuotnpseirdfeorremds.tThheeasveeprargoemhputsmaarne-rtahteenrosenle6cotiuvteloyf\nusethethreeavaaildadbeledstiozetsh:e8aBc,t6iv2eBl,iasntdba5s4e0dBo.n wh2e3thBeBrHthteaisrksscaonredsisploavceeratlhle1m.4%inbtheettetropthasnetthoef\nEvaluation pro\nc\nto\nan\nco\nd\nl\ni\n.\ndaWtees.",
      "etthoef\nEvaluation pro\nc\nto\nan\nco\nd\nl\ni\n.\ndaWtees.eAvablueaatme wallidltahnogfua1greesultBsIiGn-aBfeonccuhseredpgorrteeeddryessuelatr,cwhh,iwchhedreemaosnasntriantefisntihtee\nmodelsviagreedbyedaemcowdiindgth(i.ree.s,uteltmspienraatnureexshaamu-stiveeffbercetaodfthincfilrusdtinsegairncshtr.ucTtihoensgaonadl aisnstwoecroonpttiinounes\npling with tempteorasteuerke pimarparmoveteedr p⌧ro=mp0t)s.gWiveen thiencthoempproumtaptito.nal resources available.",
      "thiencthoempproumtaptito.nal resources available. Iterative\nextract the finalimanpsrwoevrembaesnetdseoanrckheeyswtoyrpdiscatlhlyatuseacCoomTbpinroatmiopntinogfaprfioxveiddensudmoubberleo-dfiigteitraimtiopnrosvien-\nthe language model is expected to produce (i.e., mentsforallthreemodelsinTable2.Forthebest\n“theansweris”).Wemeasureaccuracyusingexact model(Codex),CoTpromptingoutperformstheav-\nmatch(EM),computedbycomparingthegenerated eragehuman-raterscoreon17outof23tasks,com-",
      "erated eragehuman-raterscoreon17outof23tasks,com-\noutputwiththeground-truthlabel.4 paredto5outof23tasksforanswer-onlyprompt-\ning. Additionally, we see that Codex with CoT\n4 Results prompting outperforms the average human-rater\nbymorethan6%,butitstilllagsbehindthebest\n4.1 Standardanswer-onlyprompting\nhuman-raterperformancebyover20%.Thisshows\nunderestimatesmodelcapabilities\nthatlanguagemodelsarestillnotperformingatthe\nTable2summarizestheperformanceofPaLM,In- levelofexperthuman-raters.",
      "heperformanceofPaLM,In- levelofexperthuman-raters.\nstructGPT,andCodexmodelsonBBHforanswer-\nonly and CoT prompting approaches. While 4.2 Positivedeltafromchain-of-thought\nanswer-onlypromptinghasbeenusedasthestan- requiressufficientmodelscale\n4Formultiple-choicetasks,thissetupdiffersslightlyfrom Nextwestudyhowtheperformanceimprovesby\nrank/scoring classification (Brown et al., 2020; Srivastava using CoT prompting as we increase the model\netal.,2022;Lampinenetal.,2022).Weprovidealanguage\nscale.",
      "2022;Lampinenetal.,2022).Weprovidealanguage\nscale.InFigure4,weplottheperformanceofboth\nmodelwithallmultiple-choiceoptionsatonce,generatean\noutputbasedontheinput,andmeasureexactmatchaccuracy. CoT and answer-only prompting (no CoT) as a\n13006\n\n12.5 • AUTOMATICPROMPTOPTIMIZATION 15\nfunctionPROMPTOPTIMIZATION(prompts,width)returnsoptimizedprompt(s)\nactive←prompts ;Initialsetofcandidateprompts\nrepeatuntildone\nfrontier←EXPAND(active);Generatenewcandidateprompts\nforeachp∈frontier",
      "ve);Generatenewcandidateprompts\nforeachp∈frontier\nactive←ADDTOBEAM(p,active,width)\nreturnBESTOF(active)\nfunctionADDTOBEAM(state,agenda,width)returnsupdatedagenda\nifLENGTH(agenda)<widththen;Additifthere’sroom\nagenda←INSERT(state,agenda)\nelseifSCORE(state)>SCORE(WORSTOF(agenda));Additifitsbetterthan\n;thecurrentworstoption.\nagenda←REMOVE(WORSTOF(agenda))\nagenda←INSERT(state,agenda)\nreturnagenda\nFigure12.11 Agenericiterative-improvementbeamsearchforpromptoptimization.",
      "rative-improvementbeamsearchforpromptoptimization.New\npromptsaregeneratedfromcurrentonesoneachiteration.Promptsthatscorewell(fittingin\ntheagenda)arekeptaround.Whenastoppingcriteriaisreachedthebestiteminthebeamis\nreturned.\ncombinationwithafailuretoimproveaftersomeperiodtotimeasstoppingcriteria.\nThislatterisequivalenttoearlystoppingwithpatienceusedintrainingdeepneural\nnetworks.\n12.5.1 CandidateScoring\nCandidatescoringmethodsassessthelikelyperformanceofpotentialprompts,both",
      "assessthelikelyperformanceofpotentialprompts,both\nto identify promising avenues of search and to prune those that are unlikely to be\neffective. Sincecandidatescoringisembeddedintheinner-loopofthesearch, the\ncomputationalcostofscoringiscritical.\nGivenaccesstolabeledtrainingdata,candidatepromptscanbescoredbasedon\nexecution execution accuracy (Honovich et al., 2023). In this approach, candidate prompts\naccuracy\narecombinedwithinputssampledfromthetrainingdataandpassedtoanLLMfor\ndecoding.",
      "edfromthetrainingdataandpassedtoanLLMfor\ndecoding. The LLM output is evaluated against the training label using a metric\nappropriateforthetask. Inthecaseofclassification-basedtasks,thisiseffectivelya\n0/1loss—howmanyexampleswerecorrectlylabeledwiththegivenprompt. Gen-\nerativeapplicationssuchassummarizationortranslationusetask-specificsimilarity\nscoressuchasBERTScore,Bleu(Papinenietal.,2002),orROUGE(Lin,2004).",
      "TScore,Bleu(Papinenietal.,2002),orROUGE(Lin,2004).\nGiven the computational cost of issuing calls to an LLM, evaluating each can-\ndidatepromptagainstacompletetrainingsetwouldbeinfeasible. Instead,prompt\nperformanceisestimatedfromasmallsampleoftrainingdata(Pryzantetal.,2023).\n12.5.2 PromptExpansion\nPromptexpansiongeneratesvariantsofagivenprompttocreateanexpandedsetof\nneighboring prompts that may improve performance over the original. A common\nmethod is to use language models to create paraphrases.",
      "d is to use language models to create paraphrases. For example Zhou et al.\n(2023)usethefollowingmeta-prompttoelicitavariantpromptfromanoriginal:\n\n16 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\nPromptingforaVariant\nGenerateavariationofthefollowinginstructionwhilekeepingthesemanticmeaning.\nInput: {INSTRUCTION}\nOutput: {COMPLETE}\nAvariationofthismethodistotruncatethecurrentpromptatasetofrandomloca-\ntions, generatingasetofpromptprefixes.",
      "randomloca-\ntions, generatingasetofpromptprefixes. TheparaphrasingLLMisthenaskedto\ncontinueeachtheprefixestogenerateacompleteprompt.\nThis methods is an example of an uninformed search. That is, the candidate\nexpansionstep isnotdirectedtowards generating bettercandidates; candidatesare\ngenerated without regard to their quality. It is the job of the priority queue to el-\nevate improved candidates when they are found. By contrast, Prasad et al.",
      "es when they are found. By contrast, Prasad et al. (2023)\nemploy a candidate expansion technique that explicitly attempts to generate supe-\nrior prompts during the expansion process. In this approach, the current candidate\nis first applied to a sample of training examples using the execution accuracy ap-\nproach. The prompt’s performance on these examples then guides the expansion\nprocess. Specifically, incorrect examples are used to critique the original prompt",
      "examples are used to critique the original prompt\n— with the critique playing the role of a gradient for the search. The method in-\ncludesthefollowingsteps.\n1. Runthepromptonasampleoftrainingexamples,\n2. Identifyexampleswherethepromptfails,\n3. AskanLLMtoproduceacritiqueofthepromptinlightofthefailedexamples,\n4. Provide the resulting critique to an LLM, and ask it to generate improved\nprompts.\nGivenapromptandasetoffailedexamples,Prasadetal.",
      "s.\nGivenapromptandasetoffailedexamples,Prasadetal.(2023)usethefollow-\ningtemplateforaclassifiertasktosolicitcritiquesfromatargetLLM.\nCritiquingPrompt\nI’m trying to write a zero-shot classifier prompt.\nMy current prompt is: {prompt}\nBut this prompt gets the following examples wrong:\n{error string}\nGive {num feedbacks} reasons why the prompt could have\ngotten these examples wrong.\nThis model feedback is then combined with a second template to elicit improved\npromptsfromtheLLM.",
      "nd template to elicit improved\npromptsfromtheLLM.\n\n12.6 • EVALUATINGPROMPTEDLANGUAGEMODELS 17\nPromptImprovementPrompt\nI’m trying to write a zero-shot classifier. My current prompt is:\n{prompt}\nBut it gets the following examples wrong: {error str}\nBased on these examples the problem with this prompt is that {gradient}.\nBased on the above information, I wrote {steps per gradient} different\nimproved prompts. Each prompt is wrapped with <START> and <END>.",
      "ts. Each prompt is wrapped with <START> and <END>.\nThe {steps per gradient} new prompts are:\n12.6 Evaluating Prompted Language Models\nLanguagemodelsareevaluatedinmanyways. weintroducedsomeevaluationsfor\nin Section ??, including measuring the language model’s perplexity on a test set,\nevaluatingitsaccuracyonvariousNLPtasks,aswellasbenchmarksthathelpmea-\nsureefficiency, toxicity, fairness, andsoon. We’llhavefurtherdiscussionofeval-",
      "rness, andsoon. We’llhavefurtherdiscussionofeval-\nuate NLP tasks in future chapters; machine translation in Chapter 13 and question\nansweringandinformationretrievalinChapter14.\nHerewejustbrieflyshowthemechanismformeasuringaccuracyinaprompt-\nMMLU ing setup for tests that have multiple-choice questions. We show this for MMLU\n(Massive Multitask Language Understanding), a commonly-used dataset of 15908\nknowledge and reasoning questions in 57 areas including medicine, mathematics,",
      "ions in 57 areas including medicine, mathematics,\ncomputer science, law, and others. For example, here is an MMLU question from\nthemicroeconomicsdomain:1\nMMLUmicroeconomicsexample\nOneofthereasonsthatthegovernmentdiscouragesandregulatesmonopo-\nliesisthat\n(A)producersurplusislostandconsumersurplusisgained.\n(B)monopolypricesensureproductiveefficiencybutcostsocietyallocative\nefficiency.\n(C)monopolyfirmsdonotengageinsignificantresearchanddevelopment.",
      "rmsdonotengageinsignificantresearchanddevelopment.\n(D)consumersurplusislostwithhigherpricesandlowerlevelsofoutput.\nFig.12.12showsthewayMMLUturnsthesequestionsintopromptedtestsofa\nlanguagemodel,inthiscaseshowinganexamplepromptwith2demonstrations.\n12.7 Model Alignment with Human Preferences: RLHF\nand DPO\nTBD\n1 Forthoseofyouwhoseeconomicsisabitrusty,thecorrectansweris(D).\n\n18 CHAPTER12 • MODELALIGNMENT,PROMPTING,ANDIN-CONTEXTLEARNING\nMMLUmathematicsprompt",
      "PTING,ANDIN-CONTEXTLEARNING\nMMLUmathematicsprompt\nThefollowingaremultiplechoicequestionsabouthighschoolmathematics.\nHowmanynumbersareinthelist25,26,...,100?\n(A)75(B)76(C)22(D)23\nAnswer: B\nComputei+i2+i3+···+i258+i259.\n(A)-1(B)1(C)i(D)-i\nAnswer: A\nIf4daps=7yaps,and5yaps=3baps,howmanydapsequal42baps?\n(A)28(B)21(C)40(D)30\nAnswer:\nFigure12.12 Sample2-shotpromptfromMMLUtestinghigh-schoolmathematics. (The\ncorrectansweris(C)).\n12.",
      "h-schoolmathematics. (The\ncorrectansweris(C)).\n12.8 Summary\nThis chapter has explored the topic of prompting large language models to follow\ninstructions. Herearesomeofthemainpointsthatwe’vecovered:\n• Simplepromptingcanbeusedtomappracticalapplicationstoproblemsthat\ncanbesolvedbyLLMswithoutalteringthemodel.\n• Labeledexamples(demonstrations)canbeusedtoprovidefurtherguidance\ntoamodelviafew-shotlearning.",
      "rovidefurtherguidance\ntoamodelviafew-shotlearning.\n• Methods like chain-of-thought can be used to create prompts that help lan-\nguagemodelsdealwithcomplexreasoningproblems.\n• Pretrainedlanguagemodelscanbealteredtobehaveindesiredwaysthrough\nmodelalignment.\n• Onemethodformodelalignmentisinstructiontuning, inwhichthemodel\nis finetuned (using the next-word-prediction language model objective) on\na dataset of instructions together with correct responses.",
      "t of instructions together with correct responses. Instruction tuning\ndatasetsareoftencreatedbyrepurposingstandardNLPdatasetsfortaskslike\nquestionansweringormachinetranslation.\nBibliographical and Historical Notes\n\nBibliographicalandHistoricalNotes 19\nBianchi,F.,M.Suzgun,G.Attanasio,P.Rottger,D.Juraf- Olsson,C.,N.Elhage,N.Nanda,N.Joseph,N.DasSarma,\nsky,T.Hashimoto,andJ.Zou.2024. Safety-tunedLLa- T.Henighan,B.Mann,A.Askell,Y.Bai,A.Chen,etal.",
      "LLa- T.Henighan,B.Mann,A.Askell,Y.Bai,A.Chen,etal.\nMAs: Lessons from improving the safety of large lan- 2022. In-contextlearningandinductionheads. ArXiv\nguagemodelsthatfollowinstructions.ICLR. preprint.\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan, Ouyang, L., J.Wu, X.Jiang, D.Almeida, C.Wainwright,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, J.Schulman,J.Hilton,F.Kelton,L.",
      "-Voss, G. Krueger, J.Schulman,J.Hilton,F.Kelton,L.Miller,M.Simens,\nT.Henighan,R.Child,A.Ramesh,D.M.Ziegler,J.Wu, A. Askell, P. Welinder, P. Christiano, J. Leike, and\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, R.Lowe.2022. Traininglanguagemodelstofollowin-\nS.Gray, B.Chess, J.Clark, C.Berner, S.McCandlish, structionswithhumanfeedback.NeurIPS,volume35.\nA.Radford,I.Sutskever,andD.Amodei.2020.Language\nPapineni,K.,S.Roukos,T.Ward,andW.-J.Zhu.2002.Bleu:\nmodelsarefew-shotlearners.",
      ",andW.-J.Zhu.2002.Bleu:\nmodelsarefew-shotlearners.NeurIPS,volume33.\nA method for automatic evaluation of machine transla-\nCheng,M.,E.Durmus,andD.Jurafsky.2023. Markedper- tion.ACL.\nsonas:Usingnaturallanguagepromptstomeasurestereo-\nPrasad,A.,P.Hase,X.Zhou,andM.Bansal.2023. GrIPS:\ntypesinlanguagemodels.ACL.\nGradient-free, edit-basedinstructionsearchforprompt-\nCobbe, K., V. Kosaraju, M. Bavarian, M. Chen, H. Jun, inglargelanguagemodels.EACL.\nL.Kaiser,M.Plappert,J.Tworek,J.Hilton,R.",
      "els.EACL.\nL.Kaiser,M.Plappert,J.Tworek,J.Hilton,R.Nakano,\nPryzant,R.,D.Iter,J.Li,Y.Lee,C.Zhu,andM.Zeng.2023.\nC.Hesse, andJ.Schulman.2021. Trainingverifiersto\nAutomaticpromptoptimizationwith“gradientdescent”\nsolvemathwordproblems.ArXivpreprint.\nandbeamsearch.EMNLP.\nCrosbie, J. and E. Shutova. 2022. Induction heads as an\nRajpurkar,P.,R.Jia,andP.Liang.2018. Knowwhatyou\nessential mechanism for pattern matching in in-context\ndon’tknow:UnanswerablequestionsforSQuAD.ACL.\nlearning.ArXivpreprint.",
      "ablequestionsforSQuAD.ACL.\nlearning.ArXivpreprint.\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.\nElhage,N.,N.Nanda,C.Olsson,T.Henighan,N.Joseph,\nSQuAD:100,000+questionsformachinecomprehension\nB.Mann,A.Askell,Y.Bai,A.Chen,T.Conerly,N.Das-\noftext.EMNLP.\nSarma,D.Drain,D.Ganguli,Z.Hatfield-Dodds,D.Her-\nnandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, Reynolds, L. and K. McDonell. 2021. Prompt program-\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S.",
      "gram-\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan- ming for large language models: Beyond the few-shot\ndlish,andC.Olah.2021. Amathematicalframeworkfor paradigm.CHI2021.\ntransformercircuits.Whitepaper. Russell, S.andP.Norvig.2002. ArtificialIntelligence: A\nGehman, S., S. Gururangan, M. Sap, Y. Choi, and N. A. ModernApproach,2ndedition.PrenticeHall.\nSmith. 2020. RealToxicityPrompts: Evaluating neu-\nSalvetti,F.,J.B.Lowe,andJ.H.Martin.2016. Atangled\nraltoxicdegenerationinlanguagemodels.",
      "16. Atangled\nraltoxicdegenerationinlanguagemodels. Findingsof\nweb: Thefaintsignalsofdeceptionintext-boulderlies\nEMNLP.\nandtruthcorpus(BLT-C).LREC.\nHonovich, O., U. Shaham, S. R. Bowman, and O. Levy.\nSheng,E.,K.-W.Chang,P.Natarajan,andN.Peng.2019.\n2023.Instructioninduction:Fromfewexamplestonatu-\nThewomanworkedasababysitter:Onbiasesinlanguage\nrallanguagetaskdescriptions.ACL.\ngeneration.EMNLP.\nIyer, S., X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig,\nSingh, S., F. Vargus, D. D’souza, B. F.",
      "D. Simig,\nSingh, S., F. Vargus, D. D’souza, B. F. Karlsson, A. Ma-\nP.Yu,K.Shuster,T.Wang,Q.Liu,P.S.Koura,X.Li,\nhendiran, W.-Y. Ko, H. Shandilya, J. Patel, D. Mat-\nB.O’Horo,G.Pereyra,J.Wang,C.Dewan,A.Celiky-\naciunas, L. O’Mahony, M. Zhang, R. Hettiarachchi,\nilmaz, L. Zettlemoyer, and V. Stoyanov. 2022. Opt-\nJ. Wilson, M. Machado, L. S. Moura, D. Krzemin´ski,\niml: Scaling language model instruction meta learning\nH. Fadaei, I. Ergu¨n, I. Okoh, A. Alaagib, O. Mu-\nthroughthelensofgeneralization.",
      "A. Alaagib, O. Mu-\nthroughthelensofgeneralization.ArXivpreprint.\ndannayake, Z. Alyafeai, V. M. Chien, S. Ruder,\nKhattab,O.,A.Singhvi,P.Maheshwari,Z.Zhang,K.San- S.Guthikonda,E.A.Alghamdi,S.Gehrmann,N.Muen-\nthanam, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, nighoff,M.Bartolo,J.Kreutzer,A.U¨U¨stu¨n,M.Fadaee,\nH.Miller,M.Zaharia,andC.Potts.2024.DSPy:Compil- andS.Hooker.2024.Ayadataset:Anopen-accesscollec-\ningdeclarativelanguagemodelcallsintoself-improving tionformultilingualinstructiontuning.",
      "lf-improving tionformultilingualinstructiontuning.ArXivpreprint.\npipelines.ICLR.\nSuzgun, M., N. Scales, N. Scha¨rli, S. Gehrmann, Y. Tay,\nLin,C.-Y.2004. ROUGE:Apackageforautomaticevalua- H.W.Chung,A.Chowdhery,Q.Le,E.Chi,D.Zhou,and\ntionofsummaries. ACL2004WorkshoponTextSumma- J.Wei.2023. ChallengingBIG-benchtasksandwhether\nrizationBranchesOut. chain-of-thoughtcansolvethem.ACLFindings.\nLongpre,S.,L.Hou,T.Vu,A.Webson,H.W.Chung,Y.Tay,\nWang, Y., S. Mishra, P. Alipoormolabashi, Y. Kordi,\nD.Zhou,Q.V.",
      "Mishra, P. Alipoormolabashi, Y. Kordi,\nD.Zhou,Q.V.Le,B.Zoph,J.Wei,andA.Roberts.2023.\nA. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran,\nTheFlancollection: Designingdataandmethodsforef-\nA. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis,\nfectiveinstructiontuning.ICML.\nH. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia,\nMin,S.,X.Lyu,A.Holtzman,M.Artetxe,M.Lewis,H.Ha- K.Doshi,K.K.Pal,M.Patel,M.Moradshahi,M.Par-\njishirzi,andL.Zettlemoyer.2022.Rethinkingtheroleof mar, M. Purohit, N. Varshney, P.",
      "thinkingtheroleof mar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma,\ndemonstrations: Whatmakesin-contextlearningwork? R.S.Puri,R.Karia,S.Doshi,S.K.Sampat,S.Mishra,\nEMNLP. S.ReddyA,S.Patro,T.Dixit,andX.Shen.2022.Super-\nMishra,S.,D.Khashabi,C.Baral,andH.Hajishirzi.2022. NaturalInstructions: Generalization via declarative in-\nCross-task generalization via natural language crowd- structionson1600+NLPtasks.EMNLP.\nsourcinginstructions.ACL.",
      "son1600+NLPtasks.EMNLP.\nsourcinginstructions.ACL.\n\n20 Chapter12 • ModelAlignment,Prompting,andIn-ContextLearning\nWebson,A.andE.Pavlick.2022. Doprompt-basedmodels\nreallyunderstandthemeaningoftheirprompts? NAACL\nHLT.\nWei,J.,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,\nQ.V.Le,D.Zhou,etal.2022.Chain-of-thoughtprompt-\ningelicitsreasoninginlargelanguagemodels. NeurIPS,\nvolume35.\nZhou, Y., A. I. Muresanu, Z. Han, K. Paster, S. Pitis,\nH.Chan, andJ.Ba.2023.",
      "Z. Han, K. Paster, S. Pitis,\nH.Chan, andJ.Ba.2023. Largelanguagemodelsare\nhuman-level prompt engineers. The Eleventh Interna-\ntionalConferenceonLearningRepresentations."
    ],
    "page_count": 20,
    "num_chunks": 153
  }
}