# Local Prompt Agent Configuration
# Save as UTF-8

system:
  language: "en"  # Interface language: en, zh-TW, zh-CN
  theme: "light"  # UI theme: light, dark
  log_level: "INFO"  # Logging: DEBUG, INFO, WARNING, ERROR
  data_dir: "data"  # Directory for storing data

backend:
  type: "ollama"  # Backend: ollama, openai, anthropic
  base_url: "http://localhost:11434"  # Ollama URL
  model: "mistral"  # Model name
  api_key: null  # API key for OpenAI/Anthropic (set in env: OPENAI_API_KEY, ANTHROPIC_API_KEY)
  timeout: 60  # Request timeout in seconds
  max_tokens: 2048  # Maximum tokens to generate
  temperature: 0.7  # Sampling temperature (0.0-1.0)

# Example configs for different backends:
# 
# For Ollama (local, free):
#   type: "ollama"
#   model: "mistral"  # or llama2, codellama, gemma, phi
#
# For OpenAI (cloud, paid):
#   type: "openai"
#   model: "gpt-4"  # or gpt-4-turbo, gpt-3.5-turbo
#   api_key: "sk-..."  # or set OPENAI_API_KEY env var
#
# For Anthropic Claude (cloud, paid):
#   type: "anthropic"
#   model: "claude-3-opus-20240229"  # or claude-3-sonnet, claude-3-haiku
#   api_key: "sk-ant-..."  # or set ANTHROPIC_API_KEY env var

database:
  url: "sqlite+aiosqlite:///data/conversations.db"  # Database URL
  echo: false  # Echo SQL statements (for debugging)
