= PDF/Document RAG System - Detailed Design Document
:toc: left
:toclevels: 4
:sectnums:
:icons: font
:source-highlighter: rouge
:version: 1.0.0
:date: {docdate}
:author: Patrick Cheung

== Document Information

[cols="1,3"]
|===
|Version |{version}
|Date |{date}
|Author |{author}
|Status |Draft
|Related Document |specification.adoc
|===

== 1. Introduction

=== 1.1 Purpose

This document provides a detailed technical design for the PDF and document processing system with Retrieval-Augmented Generation (RAG) capabilities for the Local Prompt Agent. It expands on the functional requirements defined in the main specification and provides implementation guidance for developers.

=== 1.2 Scope

This design covers:

* Document ingestion and processing pipeline
* Text extraction from multiple document formats
* OCR integration for scanned documents
* Text chunking strategies and optimization
* Embedding generation and caching
* Vector storage and similarity search
* RAG query processing and response generation
* Performance optimization techniques
* Error handling and edge cases

=== 1.3 Goals

* *Accuracy*: High-quality text extraction and relevant chunk retrieval
* *Performance*: Fast indexing and sub-second query response times
* *Scalability*: Handle thousands of documents efficiently
* *Flexibility*: Support multiple document formats and RAG strategies
* *Privacy*: All processing occurs locally, no cloud dependencies
* *Reliability*: Robust error handling and recovery

== 2. System Architecture

=== 2.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Local Prompt Agent                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   User     â”‚â”€â”€â”€â”€â–¶â”‚  RAG System  â”‚â”€â”€â”€â”€â–¶â”‚ LLM Backend  â”‚  â”‚
â”‚  â”‚ Interface  â”‚â—€â”€â”€â”€â”€â”‚              â”‚â—€â”€â”€â”€â”€â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚                            â”‚                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â–¼                                      â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Document    â”‚                    â”‚   Vector     â”‚      â”‚
â”‚  â”‚  Processor   â”‚                    â”‚   Store      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                                   â”‚               â”‚
â”‚         â–¼                                   â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Chunker    â”‚â”€â”€â”€â”€â”€â”€Embeddingsâ”€â”€â”€â–¶â”‚  ChromaDB/   â”‚      â”‚
â”‚  â”‚              â”‚                    â”‚    FAISS     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

=== 2.2 Component Overview

==== 2.2.1 Document Processor
Handles loading and extracting content from various document formats.

*Key Responsibilities:*

* File type detection
* Format-specific parsing (PDF, DOCX, HTML, etc.)
* Metadata extraction
* Table and image extraction
* OCR integration for scanned documents
* Error handling for corrupted files

==== 2.2.2 Document Chunker
Splits documents into optimal chunks for embedding and retrieval.

*Key Responsibilities:*

* Implement multiple chunking strategies
* Maintain semantic coherence
* Handle chunk overlap for context preservation
* Optimize chunk sizes for embedding models
* Preserve document structure information

==== 2.2.3 Embedding Generator
Creates vector representations of text chunks.

*Key Responsibilities:*

* Generate embeddings using local or remote models
* Batch processing for efficiency
* Caching to avoid recomputation
* Support multiple embedding models
* Normalize embeddings for consistency

==== 2.2.4 Vector Store
Manages storage and retrieval of document embeddings.

*Key Responsibilities:*

* Store embeddings with metadata
* Perform similarity search
* Support filtered queries
* Handle updates and deletions
* Persist data for session continuity

==== 2.2.5 RAG System Orchestrator
Coordinates all components for end-to-end RAG functionality.

*Key Responsibilities:*

* Manage document indexing workflow
* Handle query processing pipeline
* Integrate with LLM backend
* Format responses with citations
* Implement different RAG strategies

=== 2.3 Data Flow

==== 2.3.1 Document Indexing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF/Doc   â”‚
â”‚    File     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load File     â”‚
â”‚ Detect Format   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Extract Text   â”‚
â”‚  Parse Tables   â”‚
â”‚  Get Metadata   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk Text     â”‚
â”‚  (Strategy-     â”‚
â”‚   based)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate       â”‚
â”‚  Embeddings     â”‚
â”‚  (Batched)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Store in       â”‚
â”‚  Vector DB      â”‚
â”‚  with Metadata  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

==== 2.3.2 Query Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚User Query   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embed Query     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Search   â”‚
â”‚ (Top-K)         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rerank Results  â”‚
â”‚ (Optional)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Format Context  â”‚
â”‚ Build Prompt    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Generate    â”‚
â”‚ Response        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add Citations   â”‚
â”‚ Return Answer   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

== 3. Detailed Component Design

=== 3.1 Document Processor

==== 3.1.1 Class Structure

```python
from typing import Protocol, Dict, Any, Optional, List
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

class DocumentFormat(Enum):
    PDF = "pdf"
    DOCX = "docx"
    TXT = "txt"
    MARKDOWN = "md"
    HTML = "html"
    RTF = "rtf"
    UNKNOWN = "unknown"

@dataclass
class DocumentMetadata:
    """Metadata extracted from document"""
    title: Optional[str] = None
    author: Optional[str] = None
    creation_date: Optional[str] = None
    modification_date: Optional[str] = None
    page_count: Optional[int] = None
    language: Optional[str] = None
    file_size: int = 0
    format: DocumentFormat = DocumentFormat.UNKNOWN

@dataclass
class TableData:
    """Represents a table extracted from document"""
    page: int
    data: List[List[str]]
    headers: Optional[List[str]] = None

@dataclass
class DocumentContent:
    """Complete document content and metadata"""
    text: str
    metadata: DocumentMetadata
    pages: Optional[List[Dict[str, Any]]] = None
    tables: Optional[List[TableData]] = None
    images: Optional[List[Dict[str, Any]]] = None
    structure: Optional[Dict[str, Any]] = None

class DocumentParser(Protocol):
    """Protocol for document parsers"""
    
    def can_parse(self, file_path: Path) -> bool:
        """Check if parser can handle this file"""
        ...
    
    def parse(self, file_path: Path) -> DocumentContent:
        """Parse document and extract content"""
        ...

class DocumentProcessor:
    """Main document processing orchestrator"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.parsers: Dict[DocumentFormat, DocumentParser] = {}
        self.max_file_size = config.get('max_file_size', 100 * 1024 * 1024)  # 100MB
        self.ocr_enabled = config.get('ocr_enabled', False)
        
        # Register parsers
        self._register_parsers()
    
    def _register_parsers(self):
        """Register all available document parsers"""
        self.parsers[DocumentFormat.PDF] = PDFParser(self.config)
        self.parsers[DocumentFormat.DOCX] = DOCXParser(self.config)
        self.parsers[DocumentFormat.TXT] = TextParser(self.config)
        self.parsers[DocumentFormat.MARKDOWN] = MarkdownParser(self.config)
        self.parsers[DocumentFormat.HTML] = HTMLParser(self.config)
    
    def process_document(self, file_path: Path) -> DocumentContent:
        """
        Process document and extract all content
        
        Args:
            file_path: Path to document file
            
        Returns:
            DocumentContent with extracted text and metadata
            
        Raises:
            FileNotFoundError: If file doesn't exist
            UnsupportedFormatError: If file format not supported
            DocumentProcessingError: If processing fails
        """
        # Validate file
        self._validate_file(file_path)
        
        # Detect format
        doc_format = self._detect_format(file_path)
        
        # Get appropriate parser
        parser = self.parsers.get(doc_format)
        if not parser:
            raise UnsupportedFormatError(f"No parser for format: {doc_format}")
        
        # Parse document
        try:
            content = parser.parse(file_path)
            return content
        except Exception as e:
            raise DocumentProcessingError(f"Failed to process {file_path}: {e}")
    
    def _validate_file(self, file_path: Path):
        """Validate file exists and is within size limits"""
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        file_size = file_path.stat().st_size
        if file_size > self.max_file_size:
            raise FileTooLargeError(
                f"File size {file_size} exceeds limit {self.max_file_size}"
            )
    
    def _detect_format(self, file_path: Path) -> DocumentFormat:
        """Detect document format from extension and magic bytes"""
        # Try extension first
        suffix = file_path.suffix.lower().lstrip('.')
        try:
            return DocumentFormat(suffix)
        except ValueError:
            pass
        
        # Try magic bytes
        with open(file_path, 'rb') as f:
            magic = f.read(4)
            
        if magic == b'%PDF':
            return DocumentFormat.PDF
        elif magic[:2] == b'PK':  # ZIP-based (DOCX, etc.)
            return DocumentFormat.DOCX
        
        return DocumentFormat.UNKNOWN
```

==== 3.1.2 PDF Parser Implementation

```python
import pdfplumber
from PyPDF2 import PdfReader
import pytesseract
from PIL import Image
from io import BytesIO

class PDFParser(DocumentParser):
    """Parser for PDF documents"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.ocr_enabled = config.get('ocr_enabled', False)
        self.ocr_language = config.get('ocr_language', 'eng')
        self.extract_tables = config.get('extract_tables', True)
        self.extract_images = config.get('extract_images', False)
    
    def can_parse(self, file_path: Path) -> bool:
        """Check if file is a PDF"""
        return file_path.suffix.lower() == '.pdf'
    
    def parse(self, file_path: Path) -> DocumentContent:
        """Parse PDF document"""
        
        # Extract metadata first
        metadata = self._extract_metadata(file_path)
        
        # Extract content
        pages_content = []
        tables = []
        images = []
        
        try:
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    # Extract text
                    text = page.extract_text()
                    
                    # Use OCR if text is sparse or missing
                    if self._should_use_ocr(text):
                        text = self._ocr_page(page)
                    
                    pages_content.append({
                        'page': page_num,
                        'text': text,
                        'width': page.width,
                        'height': page.height
                    })
                    
                    # Extract tables
                    if self.extract_tables:
                        page_tables = self._extract_tables(page, page_num)
                        tables.extend(page_tables)
                    
                    # Extract images
                    if self.extract_images:
                        page_images = self._extract_images(page, page_num)
                        images.extend(page_images)
        
        except Exception as e:
            if "password" in str(e).lower():
                raise PasswordProtectedError(f"PDF is password protected: {file_path}")
            else:
                raise PDFExtractionError(f"Failed to extract PDF content: {e}")
        
        # Combine all pages
        full_text = self._combine_pages(pages_content)
        
        return DocumentContent(
            text=full_text,
            metadata=metadata,
            pages=pages_content,
            tables=tables if self.extract_tables else None,
            images=images if self.extract_images else None
        )
    
    def _extract_metadata(self, file_path: Path) -> DocumentMetadata:
        """Extract PDF metadata"""
        try:
            reader = PdfReader(file_path)
            info = reader.metadata
            
            return DocumentMetadata(
                title=info.get('/Title', None),
                author=info.get('/Author', None),
                creation_date=info.get('/CreationDate', None),
                modification_date=info.get('/ModDate', None),
                page_count=len(reader.pages),
                file_size=file_path.stat().st_size,
                format=DocumentFormat.PDF
            )
        except Exception as e:
            # Return minimal metadata on error
            return DocumentMetadata(
                page_count=0,
                file_size=file_path.stat().st_size,
                format=DocumentFormat.PDF
            )
    
    def _should_use_ocr(self, text: Optional[str]) -> bool:
        """Determine if OCR should be used for this page"""
        if not self.ocr_enabled:
            return False
        
        if not text:
            return True
        
        # If text is very short, might be scanned
        if len(text.strip()) < 50:
            return True
        
        return False
    
    def _ocr_page(self, page) -> str:
        """Perform OCR on a PDF page"""
        try:
            # Convert page to image
            image = page.to_image(resolution=300)
            pil_image = image.original
            
            # Perform OCR
            text = pytesseract.image_to_string(
                pil_image,
                lang=self.ocr_language,
                config='--psm 6'  # Assume uniform block of text
            )
            
            return text
        except Exception as e:
            print(f"OCR failed: {e}")
            return ""
    
    def _extract_tables(self, page, page_num: int) -> List[TableData]:
        """Extract tables from PDF page"""
        tables = []
        
        try:
            page_tables = page.extract_tables()
            for table_data in page_tables:
                if table_data and len(table_data) > 0:
                    # First row might be headers
                    headers = table_data[0] if len(table_data) > 1 else None
                    data = table_data[1:] if headers else table_data
                    
                    tables.append(TableData(
                        page=page_num,
                        data=data,
                        headers=headers
                    ))
        except Exception as e:
            print(f"Table extraction failed for page {page_num}: {e}")
        
        return tables
    
    def _extract_images(self, page, page_num: int) -> List[Dict[str, Any]]:
        """Extract images from PDF page"""
        images = []
        
        try:
            for image_obj in page.images:
                images.append({
                    'page': page_num,
                    'x0': image_obj['x0'],
                    'y0': image_obj['y0'],
                    'x1': image_obj['x1'],
                    'y1': image_obj['y1'],
                    'width': image_obj['width'],
                    'height': image_obj['height']
                })
        except Exception as e:
            print(f"Image extraction failed for page {page_num}: {e}")
        
        return images
    
    def _combine_pages(self, pages_content: List[Dict[str, Any]]) -> str:
        """Combine pages with appropriate separators"""
        texts = []
        
        for page_data in pages_content:
            text = page_data['text']
            if text:
                texts.append(text)
        
        # Join with double newline to preserve page breaks
        return "\n\n".join(texts)
```

=== 3.2 Document Chunker

==== 3.2.1 Chunking Strategies

```python
from typing import List, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod
import re

@dataclass
class Chunk:
    """Represents a chunk of text"""
    text: str
    start_pos: int
    end_pos: int
    chunk_index: int
    metadata: Dict[str, Any]
    tokens: Optional[int] = None

class ChunkingStrategy(ABC):
    """Abstract base for chunking strategies"""
    
    @abstractmethod
    def chunk(self, text: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Split text into chunks"""
        pass

class FixedSizeChunker(ChunkingStrategy):
    """Fixed-size chunking with overlap"""
    
    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk(self, text: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Split text into fixed-size chunks"""
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < len(text):
            end = min(start + self.chunk_size, len(text))
            
            # Try to break at sentence boundary
            if end < len(text):
                # Look for sentence end markers
                sentence_ends = ['.', '!', '?', '\n']
                for i in range(end, max(start, end - 50), -1):
                    if text[i] in sentence_ends:
                        end = i + 1
                        break
            
            chunk_text = text[start:end]
            
            chunks.append(Chunk(
                text=chunk_text,
                start_pos=start,
                end_pos=end,
                chunk_index=chunk_index,
                metadata=metadata
            ))
            
            chunk_index += 1
            start = end - self.overlap
        
        return chunks

class SemanticChunker(ChunkingStrategy):
    """Chunk by semantic boundaries (paragraphs, sections)"""
    
    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk(self, text: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Split text by semantic boundaries"""
        # Split into paragraphs
        paragraphs = self._split_paragraphs(text)
        
        chunks = []
        current_chunk = []
        current_size = 0
        chunk_index = 0
        position = 0
        
        for para in paragraphs:
            para_size = len(para)
            
            if current_size + para_size > self.chunk_size and current_chunk:
                # Save current chunk
                chunk_text = "\n\n".join(current_chunk)
                chunks.append(Chunk(
                    text=chunk_text,
                    start_pos=position - current_size,
                    end_pos=position,
                    chunk_index=chunk_index,
                    metadata=metadata
                ))
                chunk_index += 1
                
                # Start new chunk with overlap
                if self.overlap > 0 and current_chunk:
                    overlap_text = current_chunk[-1]
                    current_chunk = [overlap_text, para]
                    current_size = len(overlap_text) + para_size
                else:
                    current_chunk = [para]
                    current_size = para_size
            else:
                current_chunk.append(para)
                current_size += para_size
            
            position += para_size
        
        # Add remaining chunk
        if current_chunk:
            chunk_text = "\n\n".join(current_chunk)
            chunks.append(Chunk(
                text=chunk_text,
                start_pos=position - current_size,
                end_pos=position,
                chunk_index=chunk_index,
                metadata=metadata
            ))
        
        return chunks
    
    def _split_paragraphs(self, text: str) -> List[str]:
        """Split text into paragraphs"""
        # Split on double newlines or more
        paragraphs = re.split(r'\n\s*\n', text)
        
        # Clean up whitespace
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        return paragraphs

class RecursiveChunker(ChunkingStrategy):
    """Recursive chunking with multiple separators"""
    
    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.separators = ["\n\n", "\n", ". ", " ", ""]
    
    def chunk(self, text: str, metadata: Dict[str, Any]) -> List[Chunk]:
        """Recursively chunk text with fallback separators"""
        chunks = []
        chunk_index = 0
        
        self._recursive_split(
            text, 
            0, 
            metadata, 
            chunk_index, 
            chunks
        )
        
        return chunks
    
    def _recursive_split(
        self,
        text: str,
        position: int,
        metadata: Dict[str, Any],
        chunk_index: int,
        chunks: List[Chunk]
    ):
        """Recursively split text"""
        if len(text) <= self.chunk_size:
            # Base case: text fits in one chunk
            chunks.append(Chunk(
                text=text,
                start_pos=position,
                end_pos=position + len(text),
                chunk_index=chunk_index,
                metadata=metadata
            ))
            return
        
        # Try each separator
        for separator in self.separators:
            if separator in text:
                # Split on this separator
                parts = text.split(separator)
                
                current_chunk = []
                current_size = 0
                
                for part in parts:
                    part_with_sep = part + separator if separator else part
                    part_size = len(part_with_sep)
                    
                    if current_size + part_size > self.chunk_size and current_chunk:
                        # Save current chunk
                        chunk_text = "".join(current_chunk)
                        chunks.append(Chunk(
                            text=chunk_text,
                            start_pos=position,
                            end_pos=position + len(chunk_text),
                            chunk_index=len(chunks),
                            metadata=metadata
                        ))
                        
                        position += len(chunk_text)
                        current_chunk = [part_with_sep]
                        current_size = part_size
                    else:
                        current_chunk.append(part_with_sep)
                        current_size += part_size
                
                # Add remaining
                if current_chunk:
                    chunk_text = "".join(current_chunk)
                    chunks.append(Chunk(
                        text=chunk_text,
                        start_pos=position,
                        end_pos=position + len(chunk_text),
                        chunk_index=len(chunks),
                        metadata=metadata
                    ))
                
                return
        
        # Fallback: force split
        mid = self.chunk_size
        chunks.append(Chunk(
            text=text[:mid],
            start_pos=position,
            end_pos=position + mid,
            chunk_index=len(chunks),
            metadata=metadata
        ))
        
        self._recursive_split(
            text[mid - self.overlap:],
            position + mid - self.overlap,
            metadata,
            chunk_index + 1,
            chunks
        )
```

=== 3.3 Embedding Generator

```python
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Optional
import hashlib
from functools import lru_cache

class EmbeddingGenerator:
    """Generate embeddings for text chunks"""
    
    def __init__(
        self,
        model_name: str = 'all-MiniLM-L6-v2',
        device: str = 'cpu',
        cache_size: int = 10000
    ):
        self.model_name = model_name
        self.device = device
        self.model = SentenceTransformer(model_name, device=device)
        self.dimension = self.model.get_sentence_embedding_dimension()
        self.cache_size = cache_size
        
        # In-memory cache
        self.cache: Dict[str, np.ndarray] = {}
    
    def embed_text(self, text: str) -> np.ndarray:
        """Generate embedding for single text"""
        # Check cache
        text_hash = self._hash_text(text)
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # Generate embedding
        embedding = self.model.encode(text, convert_to_numpy=True)
        
        # Cache result
        self._cache_embedding(text_hash, embedding)
        
        return embedding
    
    def embed_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        show_progress: bool = False
    ) -> List[np.ndarray]:
        """Generate embeddings for multiple texts efficiently"""
        
        # Check cache for each text
        embeddings = []
        uncached_indices = []
        uncached_texts = []
        
        for i, text in enumerate(texts):
            text_hash = self._hash_text(text)
            if text_hash in self.cache:
                embeddings.append(self.cache[text_hash])
            else:
                embeddings.append(None)
                uncached_indices.append(i)
                uncached_texts.append(text)
        
        # Generate embeddings for uncached texts
        if uncached_texts:
            new_embeddings = self.model.encode(
                uncached_texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                convert_to_numpy=True
            )
            
            # Insert into results and cache
            for idx, embedding in zip(uncached_indices, new_embeddings):
                embeddings[idx] = embedding
                text_hash = self._hash_text(texts[idx])
                self._cache_embedding(text_hash, embedding)
        
        return embeddings
    
    def _hash_text(self, text: str) -> str:
        """Generate hash for text"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _cache_embedding(self, text_hash: str, embedding: np.ndarray):
        """Cache embedding with size limit"""
        if len(self.cache) >= self.cache_size:
            # Remove oldest entry (simple FIFO)
            self.cache.pop(next(iter(self.cache)))
        
        self.cache[text_hash] = embedding
    
    def get_dimension(self) -> int:
        """Get embedding dimension"""
        return self.dimension
```

=== 3.4 Vector Store

```python
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Optional
import numpy as np

class VectorStore:
    """Manages vector storage and similarity search"""
    
    def __init__(
        self,
        collection_name: str,
        persist_directory: Optional[str] = None,
        embedding_function: Optional[Any] = None
    ):
        self.collection_name = collection_name
        
        # Initialize ChromaDB
        if persist_directory:
            self.client = chromadb.PersistentClient(path=persist_directory)
        else:
            self.client = chromadb.Client()
        
        # Get or create collection
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # Use cosine similarity
        )
    
    def add_documents(
        self,
        chunks: List[str],
        embeddings: List[np.ndarray],
        metadatas: List[Dict[str, Any]],
        ids: Optional[List[str]] = None
    ):
        """Add document chunks with embeddings to vector store"""
        
        # Generate IDs if not provided
        if ids is None:
            ids = [f"{self.collection_name}_{i}" for i in range(len(chunks))]
        
        # Convert numpy arrays to lists for ChromaDB
        embeddings_list = [emb.tolist() for emb in embeddings]
        
        # Add to collection
        self.collection.add(
            ids=ids,
            embeddings=embeddings_list,
            documents=chunks,
            metadatas=metadatas
        )
    
    def search(
        self,
        query_embedding: np.ndarray,
        k: int = 5,
        where: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Similarity search for query"""
        
        # Convert numpy array to list
        query_embedding_list = query_embedding.tolist()
        
        # Query collection
        results = self.collection.query(
            query_embeddings=[query_embedding_list],
            n_results=k,
            where=where
        )
        
        # Format results
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'id': results['ids'][0][i],
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            })
        
        return formatted_results
    
    def delete_collection(self):
        """Delete the entire collection"""
        self.client.delete_collection(name=self.collection_name)
    
    def count(self) -> int:
        """Get number of documents in collection"""
        return self.collection.count()
```

=== 3.5 RAG System

```python
from typing import List, Dict, Any, Optional
from pathlib import Path

class RAGSystem:
    """Complete RAG system orchestration"""
    
    def __init__(
        self,
        config: Dict[str, Any],
        llm_backend: Any
    ):
        self.config = config
        self.llm_backend = llm_backend
        
        # Initialize components
        self.document_processor = DocumentProcessor(config)
        
        chunking_strategy = config.get('chunking_strategy', 'semantic')
        chunk_size = config.get('chunk_size', 500)
        chunk_overlap = config.get('chunk_overlap', 50)
        
        if chunking_strategy == 'fixed':
            self.chunker = FixedSizeChunker(chunk_size, chunk_overlap)
        elif chunking_strategy == 'semantic':
            self.chunker = SemanticChunker(chunk_size, chunk_overlap)
        elif chunking_strategy == 'recursive':
            self.chunker = RecursiveChunker(chunk_size, chunk_overlap)
        
        self.embedding_generator = EmbeddingGenerator(
            model_name=config.get('embedding_model', 'all-MiniLM-L6-v2'),
            device=config.get('device', 'cpu')
        )
        
        self.vector_store = VectorStore(
            collection_name=config.get('collection_name', 'documents'),
            persist_directory=config.get('persist_directory')
        )
    
    def index_document(self, file_path: Path) -> Dict[str, Any]:
        """Index a document for RAG"""
        
        print(f"ðŸ“„ Processing document: {file_path.name}")
        
        # Step 1: Process document
        document = self.document_processor.process_document(file_path)
        print(f"âœ“ Extracted {len(document.text)} characters")
        
        # Step 2: Chunk document
        chunks = self.chunker.chunk(document.text, document.metadata.__dict__)
        print(f"âœ“ Created {len(chunks)} chunks")
        
        # Step 3: Generate embeddings
        chunk_texts = [chunk.text for chunk in chunks]
        embeddings = self.embedding_generator.embed_batch(
            chunk_texts,
            show_progress=True
        )
        print(f"âœ“ Generated {len(embeddings)} embeddings")
        
        # Step 4: Store in vector database
        metadatas = [
            {
                'source': str(file_path),
                'chunk_index': chunk.chunk_index,
                'start_pos': chunk.start_pos,
                'end_pos': chunk.end_pos,
                **chunk.metadata
            }
            for chunk in chunks
        ]
        
        self.vector_store.add_documents(
            chunks=chunk_texts,
            embeddings=embeddings,
            metadatas=metadatas
        )
        print(f"âœ“ Stored in vector database")
        
        return {
            'file_path': str(file_path),
            'num_chunks': len(chunks),
            'num_pages': document.metadata.page_count or 'N/A',
            'file_size': document.metadata.file_size
        }
    
    def query(
        self,
        question: str,
        k: int = 5,
        mode: str = 'simple'
    ) -> Dict[str, Any]:
        """Query the RAG system"""
        
        if mode == 'simple':
            return self._simple_rag(question, k)
        elif mode == 'multi_query':
            return self._multi_query_rag(question, k)
        else:
            raise ValueError(f"Unknown mode: {mode}")
    
    def _simple_rag(self, question: str, k: int) -> Dict[str, Any]:
        """Simple RAG: retrieve and generate"""
        
        # Step 1: Embed query
        query_embedding = self.embedding_generator.embed_text(question)
        
        # Step 2: Retrieve relevant chunks
        results = self.vector_store.search(query_embedding, k=k)
        
        if not results:
            return {
                'answer': "I don't have any information to answer this question.",
                'sources': [],
                'confidence': 0.0
            }
        
        # Step 3: Format context
        context = self._format_context(results)
        
        # Step 4: Generate response with LLM
        prompt = f"""Answer the question based on the following context. If the context doesn't contain relevant information, say so.

Context:
{context}

Question: {question}

Answer:"""
        
        response = self.llm_backend.complete(prompt)
        
        # Step 5: Add citations
        sources = self._extract_sources(results)
        
        return {
            'answer': response,
            'sources': sources,
            'retrieved_chunks': len(results),
            'confidence': self._estimate_confidence(results)
        }
    
    def _format_context(self, results: List[Dict[str, Any]]) -> str:
        """Format retrieved chunks into context string"""
        context_parts = []
        
        for i, result in enumerate(results, 1):
            source = Path(result['metadata']['source']).name
            chunk_index = result['metadata'].get('chunk_index', 'N/A')
            
            context_parts.append(
                f"[{i}] (Source: {source}, Chunk: {chunk_index})\n{result['text']}"
            )
        
        return "\n\n".join(context_parts)
    
    def _extract_sources(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract unique sources from results"""
        sources_dict = {}
        
        for result in results:
            source_path = result['metadata']['source']
            if source_path not in sources_dict:
                sources_dict[source_path] = {
                    'file': Path(source_path).name,
                    'path': source_path,
                    'chunks_used': 0
                }
            sources_dict[source_path]['chunks_used'] += 1
        
        return list(sources_dict.values())
    
    def _estimate_confidence(self, results: List[Dict[str, Any]]) -> float:
        """Estimate confidence based on retrieval quality"""
        if not results:
            return 0.0
        
        # Use average distance (lower is better for cosine distance)
        distances = [r.get('distance', 1.0) for r in results]
        avg_distance = sum(distances) / len(distances)
        
        # Convert to confidence (0-1 scale)
        # For cosine distance, 0 = identical, 2 = opposite
        confidence = 1.0 - (avg_distance / 2.0)
        
        return max(0.0, min(1.0, confidence))
```

== 4. Performance Optimization

=== 4.1 Batch Processing

* Process documents in batches to maximize GPU utilization
* Use async I/O for reading multiple files
* Parallel embedding generation for multiple chunks

=== 4.2 Caching Strategy

* Cache document text extractions
* Cache embeddings with hash-based invalidation
* Cache query results for repeated questions

=== 4.3 Index Optimization

* Use approximate nearest neighbor search (HNSW)
* Quantize embeddings for reduced memory
* Implement index sharding for large collections

=== 4.4 Memory Management

* Stream large documents instead of loading entirely
* Lazy load vector indices
* Implement memory-mapped file access

== 5. Configuration Example

```yaml
# RAG Configuration
rag:
  # Document Processing
  max_file_size: 104857600  # 100MB
  ocr_enabled: false
  ocr_language: "eng"
  extract_tables: true
  extract_images: false
  
  # Chunking
  chunking_strategy: "semantic"  # fixed, semantic, recursive
  chunk_size: 500
  chunk_overlap: 50
  
  # Embeddings
  embedding_model: "all-MiniLM-L6-v2"
  device: "cpu"  # or "cuda"
  embedding_cache_size: 10000
  batch_size: 32
  
  # Vector Store
  collection_name: "documents"
  persist_directory: "~/.local-prompt-agent/vector_store"
  similarity_metric: "cosine"  # cosine, euclidean, dot_product
  
  # Retrieval
  default_k: 5
  max_k: 20
  min_similarity_threshold: 0.5
  
  # RAG Modes
  default_mode: "simple"  # simple, multi_query
  enable_reranking: false
  reranker_model: null
```

== 6. API Endpoints

```
POST /api/v1/rag/index
  Body: {
    "file_path": "/path/to/document.pdf",
    "collection": "my_documents"
  }
  
POST /api/v1/rag/query
  Body: {
    "question": "What is the main topic?",
    "collection": "my_documents",
    "k": 5,
    "mode": "simple"
  }

GET /api/v1/rag/collections
  Returns: List of available collections

DELETE /api/v1/rag/collections/{name}
  Deletes a collection

GET /api/v1/rag/stats
  Returns: Statistics about indexed documents
```

== 7. CLI Commands

```bash
# Index a document
agent rag index document.pdf --collection research

# Query documents
agent rag query "What are the key findings?" --collection research

# List collections
agent rag list

# Delete collection
agent rag delete research

# Show statistics
agent rag stats

# Reindex collection
agent rag reindex research
```

== 8. Testing Strategy

=== 8.1 Unit Tests
* Test each component independently
* Mock external dependencies
* Test edge cases and error conditions

=== 8.2 Integration Tests
* Test end-to-end workflows
* Test with real documents
* Verify retrieval quality

=== 8.3 Performance Tests
* Measure indexing speed
* Measure query latency
* Test with large document collections

== 9. Deployment Considerations

=== 9.1 Resource Requirements

* *CPU*: 2+ cores recommended
* *RAM*: 4GB minimum, 8GB+ recommended
* *Disk*: Depends on document collection size
* *GPU*: Optional, speeds up embedding generation

=== 9.2 Scaling Strategies

* Horizontal: Multiple instances with load balancer
* Vertical: Larger machine for bigger collections
* Distributed: Shard collections across nodes

== 10. Future Enhancements

* Support for more document formats (Excel, PowerPoint)
* Multi-modal RAG (images, tables as visual elements)
* Advanced reranking models
* Hybrid search combining BM25 and semantic
* Query understanding and reformulation
* Conversation-aware RAG
* Citation verification and fact-checking

---

*END OF DESIGN DOCUMENT*

*Version: {version}*
*Last Updated: {date}*
